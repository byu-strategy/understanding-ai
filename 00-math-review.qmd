---
title: "Math and Python Review"
format: html
editor: visual
---

This guide introduces the key concepts in **calculus**, **linear algebra**, **probability and statistics**, and **Python programming** that you’ll need to understand how neural networks and large language models (LLMs) work.

---

## Calculus Essentials

### Why You Need Calculus in ML/AI

Neural networks are functions trained to minimize prediction errors. To train them, we need to compute how changes in weights affect the output — and for that, we use differentiation and the chain rule.

### Derivatives

A measure of how a function changes as its input changes.

**Notation**:  
- $ \frac{dy}{dx} $: derivative of output $y$ with respect to input $x$  
- $ f'(x) $: shorthand for “the derivative of function $f$ at $x$”

**Example**:  
If $f(x) = x^2$, then $f'(x) = 2x$

### Partial Derivatives

A derivative with respect to one variable while keeping others constant.

**Notation**:  
$ \frac{\partial L}{\partial w} $

Used in computing how the loss changes with respect to each model parameter.

### Chain Rule

Used to compute derivatives of composed functions (e.g., layer-by-layer in a neural network).

**Formula**:  
$$
\frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}
$$

### Gradient

A vector of all partial derivatives of a function with respect to each input.

**Notation**:  
$ \nabla L = \left[ \frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2}, \ldots \right] $

### Loss Function

Measures how wrong the model’s prediction is.

**Examples**:  
- Mean Squared Error (MSE)  
- Cross-Entropy Loss

### Backpropagation

An algorithm that uses the chain rule to compute gradients efficiently in neural networks.

### Gradient Descent

Method used to update weights based on gradients.

**Update Rule**:  
$$
w := w - \eta \cdot \frac{\partial L}{\partial w}
$$

Where $ \eta $ is the learning rate.

---

## Linear Algebra Essentials

### Why You Need Linear Algebra in ML/AI

Neural networks use vectors and matrices to represent data, weights, and transformations.

### Vectors

A 1D array of numbers.  
$ \vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} $

### Matrices

A 2D array of numbers.  
$ A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} $

### Matrix-Vector Multiplication

If $A$ is $m \times n$ and $\vec{x}$ is $n \times 1$, then $A \vec{x}$ is $m \times 1$.

### Dot Product

$ \vec{a} \cdot \vec{b} = \sum_{i=1}^{n} a_i b_i $

Used in similarity measures and basic model computations.

### Transpose

Switches rows and columns in a matrix.  
$ A^T $

### Identity Matrix

Square matrix with 1s on the diagonal.  
Acts like “1” for matrix multiplication.  
$ AI = A $

### Matrix Multiplication

Combines transformations.  
If $A$ is $m \times n$ and $B$ is $n \times p$, then $C = AB$ is $m \times p$.

### Norms

Measure the size or length of a vector.

**L2 norm**:  
$ \| \vec{v} \|_2 = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2} $

### Eigenvectors and Eigenvalues (Optional)

Satisfy:  
$ A \vec{v} = \lambda \vec{v} $

Used in theoretical understanding and dimensionality reduction.

---

## Probability and Statistics Essentials

### Why You Need Probability in ML/AI

Models make predictions under uncertainty. Probability describes this uncertainty and informs how we evaluate and train models.

### Random Variables

Represent outcomes of random processes.

Discrete: number of heads  
Continuous: model confidence score

### Probability Distributions

Discrete: $P(X = x)$  
Continuous: $p(x)$

**Examples**:  
- Bernoulli (binary outcomes)  
- Categorical (multi-class)  
- Gaussian/Normal (real-valued data)

### Expectation (Mean)

**Discrete**:  
$ \mathbb{E}[X] = \sum_x x \cdot P(X = x) $

**Continuous**:  
$ \mathbb{E}[X] = \int x \cdot p(x) dx $

### Variance and Standard Deviation

$ \mathrm{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] $  
$ \sigma = \sqrt{\mathrm{Var}(X)} $

### Conditional Probability

$ P(A \mid B) = \frac{P(A \cap B)}{P(B)} $

Used in next-token prediction:  
$P(\text{next token} \mid \text{context})$

### Bayes’ Theorem

$ P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)} $

### Entropy

$ H(X) = -\sum_x P(x) \log P(x) $

Measures uncertainty in a distribution.

### Cross-Entropy Loss

$ \text{Loss} = -\sum_i y_i \log(\hat{y}_i) $

Used in classification and language modeling.

---

## Python Essentials for Building a Language Model

### Why Python?

Python is widely used in AI due to its readability and powerful libraries like NumPy and PyTorch.

### Variables and Types

```python
x = 5        # integer
y = 3.14     # float
word = "GPT" # string
