---
title: "Notes"
editor: visual
---

Week 1: Introduction to LLMs & the Big Picture
What is a language model?
History: From n-grams to transformers
Overview of transformer-based LLMs (GPT, BERT)
Course objectives and deliverables
Set up Python environment (Jupyter/Colab)
ğŸ§® Week 2: Tokenization & Language as Numbers
Subword tokenization (Byte Pair Encoding or Unigram LM)
Building a simple tokenizer
Vocabulary, tokens, and numerical representations
Padding, special tokens, masks
Project: Implement a tokenizer + detokenizer

ğŸ”£ Week 3: Building Blocks of Neural Nets
Vectors, matrices, and basic operations
Activation functions (ReLU, GELU, Softmax)
Forward pass and backpropagation
Loss functions (cross-entropy)
Project: Train a 2-layer feedforward model on character-level prediction

ğŸ“ Week 4: Attention is All You Need
Sequence models: RNNs vs Attention
Attention mechanism (Scaled Dot-Product Attention)
Multi-head attention
Positional encodings
Project: Implement attention from scratch

ğŸ§± Week 5: Transformer Architecture
Encoder vs Decoder stack (focus on Decoder-only for GPT-style)
LayerNorm, residuals, feedforward layers
Putting it together: a single transformer block
Project: Build a transformer block from scratch

ğŸ“š Week 6: Training a Mini GPT
Create dataset (e.g. TinyShakespeare, small dialogues)
Causal language modeling (next-token prediction)
Training loop with mini-batches
Overfitting a tiny model on a small dataset
Project: Train your toy GPT on simple data

ğŸ§ª Week 7: Sampling & Text Generation
Temperature, top-k, nucleus sampling
Greedy vs stochastic decoding
Generating completions and prompts
Project: Generate coherent completions from your model

ğŸ§  Week 8: Scaling Up and Limitations
Why scale matters: parameters vs performance
Memory and compute tradeoffs
Basics of optimization (Adam, gradient clipping, learning rate schedules)
Optional Projects:

Add dropout, masking
Implement weight sharing or rotary positional embeddings
ğŸ§© Week 9â€“10: Stretch Topics (Pick based on student interest)
Fine-tuning vs Pretraining
RLHF and human alignment
Embeddings and prompt tuning
Distillation and quantization
ğŸ“ˆ Week 11: Evaluation and Interpretability
Perplexity, BLEU, qualitative review
Basic probing of neurons
Visualization of attention heads