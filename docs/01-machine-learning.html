<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Machine Learning – STRAT 490R - Understanding AI: From Foundations to Strategy</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02-deep-learning.html" rel="next">
<link href="./00-math-review.html" rel="prev">
<link href="./images/understanding-ai-favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-machine-learning.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Machine Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STRAT 490R - Understanding AI: From Foundations to Strategy</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Syllabus</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-math-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Math and Python Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-machine-learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-deep-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-llms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-strategy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AI Strategy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./90-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./98-faq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">FAQ</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#machine-learning-in-a-nutshell" id="toc-machine-learning-in-a-nutshell" class="nav-link active" data-scroll-target="#machine-learning-in-a-nutshell"><span class="header-section-number">3.1</span> Machine Learning in a Nutshell</a></li>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression"><span class="header-section-number">3.2</span> Simple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#mathematical-model" id="toc-mathematical-model" class="nav-link" data-scroll-target="#mathematical-model"><span class="header-section-number">3.2.1</span> Mathematical model</a></li>
  <li><a href="#training-data" id="toc-training-data" class="nav-link" data-scroll-target="#training-data"><span class="header-section-number">3.2.2</span> Training Data</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function"><span class="header-section-number">3.2.3</span> Loss Function</a></li>
  <li><a href="#learning-weight-values" id="toc-learning-weight-values" class="nav-link" data-scroll-target="#learning-weight-values"><span class="header-section-number">3.2.4</span> Learning weight values</a></li>
  </ul></li>
  <li><a href="#glossary" id="toc-glossary" class="nav-link" data-scroll-target="#glossary"><span class="header-section-number">3.3</span> Glossary</a>
  <ul class="collapse">
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example"><span class="header-section-number">3.3.1</span> Example</a></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">3.3.2</span> Multiple Linear Regression</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">3.3.3</span> Logistic Regression</a></li>
  <li><a href="#classification" id="toc-classification" class="nav-link" data-scroll-target="#classification"><span class="header-section-number">3.3.4</span> Classification</a></li>
  </ul></li>
  <li><a href="#loss-functions" id="toc-loss-functions" class="nav-link" data-scroll-target="#loss-functions"><span class="header-section-number">3.4</span> Loss Functions</a></li>
  <li><a href="#gradient-descent-1" id="toc-gradient-descent-1" class="nav-link" data-scroll-target="#gradient-descent-1"><span class="header-section-number">3.5</span> Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#supervised-learning-use-cases" id="toc-supervised-learning-use-cases" class="nav-link" data-scroll-target="#supervised-learning-use-cases"><span class="header-section-number">3.5.1</span> Supervised Learning Use Cases</a></li>
  </ul></li>
  <li><a href="#unsupervised-learning" id="toc-unsupervised-learning" class="nav-link" data-scroll-target="#unsupervised-learning"><span class="header-section-number">3.6</span> Unsupervised Learning</a>
  <ul class="collapse">
  <li><a href="#unsupervised-learning-use-cases" id="toc-unsupervised-learning-use-cases" class="nav-link" data-scroll-target="#unsupervised-learning-use-cases"><span class="header-section-number">3.6.1</span> Unsupervised Learning Use Cases</a></li>
  </ul></li>
  <li><a href="#reinforcement-learning" id="toc-reinforcement-learning" class="nav-link" data-scroll-target="#reinforcement-learning"><span class="header-section-number">3.7</span> Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#reinforcement-learning-use-cases" id="toc-reinforcement-learning-use-cases" class="nav-link" data-scroll-target="#reinforcement-learning-use-cases"><span class="header-section-number">3.7.1</span> Reinforcement Learning Use Cases</a></li>
  </ul></li>
  <li><a href="#machine-learning" id="toc-machine-learning" class="nav-link" data-scroll-target="#machine-learning"><span class="header-section-number">3.8</span> Machine Learning</a></li>
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning"><span class="header-section-number">3.9</span> Supervised Learning</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Machine Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="under-construction-banner">
<p><img src="images/under-construction.png" alt="Course Under Construction" class="under-construction-image"></p>
</div>
<style>
.under-construction-banner {
  background-color: #e0e0e0;  /* Light grey background */
  padding: 1rem 1rem;         /* Makes the banner tall */
  text-align: center;
}

.under-construction-image {
  max-width: 250px;           /* Shrinks the image */
  width: 100%;
  height: auto;
  margin: 0 auto;
  border-radius: 8px;
  box-shadow: 2px 2px 8px rgba(0,0,0,0.15);
}
</style>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-ml-map.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Image source: <em>Build a Large Language Model (From Scratch)</em> by Sebastian Raschka</figcaption>
</figure>
</div>
<p>According to John D. Kelleher “the field of artificial intelligence was born at a workshop at Dartmouth College in the summer of 1956.”</p>
<section id="machine-learning-in-a-nutshell" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="machine-learning-in-a-nutshell"><span class="header-section-number">3.1</span> Machine Learning in a Nutshell</h2>
<p>Machine learning is about enabling computers to recognize patterns in data so they can make accurate predictions or decisions without being explicitly programmed for every scenario. At its core, it involves feeding a model real-world examples encoded in a dataset and allowing the computer to learn the relationship between inputs and outputs. Once trained, the model can apply that learned relationship to make predictions on new, unseen data.</p>
<p>To build a machine learning model, we need four key ingredients:</p>
<ol type="1">
<li><strong>A mathematical model:</strong> This defines the form of the mathematical function we’ll use to relate inputs to outputs, for example, a straight line (linear) or a more flexible structure like a neural network (non-linear).</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What do we mean by “linear”?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>“Linear” in this context means that the effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(\hat{y}\)</span> is <strong>proportional and constant</strong>: no matter what value of <span class="math inline">\(x\)</span> we choose, an increase of 1 unit in <span class="math inline">\(x\)</span> always increases <span class="math inline">\(\hat{y}\)</span> by exactly <span class="math inline">\(w_1\)</span> units. This property makes the model highly interpretable.</p>
</div>
</div>
</div>
<ol start="2" type="1">
<li><strong>Training data:</strong> A collection of real-world examples that pair inputs with their corresponding outputs. The quality and relevance of this data are crucial to how well the model can learn and make accurate predictions.</li>
<li><strong>A loss function:</strong> A mathematical expression that measures how far off the model’s predictions are from the correct answers. It provides feedback to help the model improve over time.</li>
<li><strong>A training algorithm:</strong> A step-by-step procedure that combines the first three ingredients in a way that minimizes the prediction errors produced by the model. This is where the so-called <em>learning</em> takes place.</li>
</ol>
<p>So what exactly is being <em>learned</em>? The computer is <em>learning</em> the <em>weights</em> (parameters) of the chosen model. This is best understood through an example. We’ll start with the simplest possible machine learning model: simple linear regression. It’s a powerful tool that helps us understand the core ideas behind more complex models, including the ones that power today’s cutting-edge AI systems like ChatGPT.</p>
</section>
<section id="simple-linear-regression" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="simple-linear-regression"><span class="header-section-number">3.2</span> Simple Linear Regression</h2>
<section id="mathematical-model" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="mathematical-model"><span class="header-section-number">3.2.1</span> Mathematical model</h3>
<p>A simple linear regression model takes a single <strong>input variable</strong> <span class="math inline">\(x\)</span> and predicts the value of a corresponding <strong>output variable</strong> <span class="math inline">\(y\)</span>. For example, the input variable might represent a house’s <em>square footage</em>, and the output variable could represent the <em>value</em> of the home.</p>
<p>We can write down the relationship between <em>square footage</em> and <em>value</em> in the form of a mathematical equation (also called a mathematical <em>function</em> or a <em>model</em>):</p>
<p><span class="math display">\[
y = w_0 + w_1 x
\]</span></p>
<p>Where <span class="math inline">\(y\)</span> represents the home <em>value</em> and <span class="math inline">\(x\)</span> represents the <em>square footage</em>.</p>
<p>In machine learning jargon <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> are called the <strong>parameters</strong> or <strong>weights</strong> of the model (hence the use of <span class="math inline">\(w\)</span> in the notation) and describe the nature of the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> are the numbers that the computer will <em>learn</em> (i.e.&nbsp;derive) based on what is observed in real life which will be encoded into <em>training data</em> discussed in the next section.</p>
<p>In other fields such as econometrics these might be introduced using the greek alphabet notation of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p>You might recognize this equation as the equation for a line in <em>slope-intercept</em> from which is often presented as:</p>
<p><span class="math display">\[
y = mx + b
\]</span></p>
<p>where <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are numbers in the coordinate plane with <span class="math inline">\(m\)</span> representing the slope of the line and <span class="math inline">\(b\)</span> representing the y-intercept (the point where the line crosses the y-axis) as shown in the image below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/line-graph.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>Simple linear regression and the equation of a line are in fact the same mathematical equation. In the context of linear regression, we simply use different notation: <span class="math inline">\(w_1 = m\)</span> (the slope), and <span class="math inline">\(w_0 = b\)</span> (the y-intercept which in machine learning is referred to as the <em>bias</em> term).</p>
<p>It’s common for different fields of study to use different notation and words for the same mathematical concepts. Unfortunately this can be one of the biggest sources of confusion for students so we will make an effort to call out these differences throughout the course.</p>
<p>Now that we’ve shown that simple linear regression is actually just the equation of a line where <span class="math inline">\(w_0\)</span> is the y-intercept and <span class="math inline">\(w_1\)</span> is the slope we can have a visual picture in mind for how an input variable <span class="math inline">\(x\)</span> and the output variable <span class="math inline">\(y\)</span> are related.</p>
<div id="fig-slr-line" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-slr-line-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/slr-line.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption><strong>Simple Linear Regression Line</strong></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-slr-line-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuition check
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>What does your intuition tell you about the values <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> are likely to take on once they are estimated in our home value example?</p>
<p>Do you expect them to be positive or negative numbers?</p>
<p>Assume <span class="math inline">\(x\)</span> was <span class="math inline">\(0\)</span>, what would the equation be telling you?</p>
</div>
</div>
</div>
<p>As soon as we come up with values for <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> have a way predicting values of <span class="math inline">\(y\)</span> when plugging in any value of <span class="math inline">\(x\)</span> to the fitted equation.</p>
<p>The goal of machine learning is to <em>learn</em> the best possible values <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> allowing us to make good predictions of a homes <em>value</em> based on it’s <em>square footage</em>. We will soon explore how the computer learns these weight values but to build our intution on what the model does we can start by simply guessing values for the weights. For example, let’s assume:</p>
<p><span class="math display">\[
w_0 = 50,\!000 \quad \text{and} \quad w_1 = 200
\]</span></p>
<p>Then our function becomes:</p>
<p><span class="math display">\[
\hat{y} = 50,\!000 + 200x
\]</span></p>
<p>We use the notation <span class="math inline">\(\hat{y}\)</span> (read as “y-hat”) here to emphasize that this is a now predicted value based on the model, not an observed or actual value.</p>
<p>This act of using the trained model to compute a prediction based on an input <span class="math inline">\(x\)</span> is also called <strong>inference</strong> since we are <em>inferring</em> an estimated output <span class="math inline">\(\hat{y}\)</span>.</p>
<p>Using this model we can predict that a home with 3,000 square feet will have a value of:</p>
<p><span class="math display">\[
\hat{y} = 50,\!000 + 200 \cdot 3,\!000 = \$650,\!000
\]</span></p>
<p>Now, consider what the model predicts for a home with 0 square feet:</p>
<p><span class="math display">\[
\hat{y} = 50,\!000 + 200 \cdot 0 = \$50,\!000
\]</span></p>
<p>This implies that the base value of the property—the land alone, with no house—might be interpreted as $50,000. This is exactly why both <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> are necessary. If we had included only <span class="math inline">\(w_1 x\)</span> and omitted <span class="math inline">\(w_0\)</span>, the model would always predict 0 for an input of <span class="math inline">\(x = 0\)</span>, which might not reflect the reality (e.g., land still has value).</p>
<p>In machine learning, the term <strong>bias</strong> is used to refer to this <span class="math inline">\(w_0\)</span> value. The name comes from the fact that it shifts (or “biases”) the entire output of the model up or down, independent of the input. Geometrically, it determines the <span class="math inline">\(y\)</span>-intercept of the prediction line. It allows the model to better fit real-world data.</p>
<p>Choosing a different set of weight values would result in a different equation, resulting in a different prediction. For example, assume instead that <span class="math inline">\(w_0 = 25,000\)</span> and <span class="math inline">\(w_1 = 300\)</span> resulting in the following equation:</p>
<p><span class="math display">\[
\hat{y} = 25,000 + 300 x
\]</span></p>
<p>This model would predict that the same 3,000 square-foot home has a much higher value of <span class="math inline">\(\$925,000 = 25,000+300*3,000\)</span>.</p>
<p>Let’s assume the $3,000 square foot home we have in mind recently sold for $800,000 reflecting it’s true value (a single instance of <span class="math inline">\(y\)</span>). We could then compute the error associated with each set of model weights as the asbolute value of the prediction error as follows:</p>
<ol type="1">
<li>Prediction error when <span class="math inline">\(w_0 = 50,\!000\)</span> and <span class="math inline">\(w_1 = 200\)</span>: <span class="math inline">\(\left| 650,000 - 8000000 \right| = 150,000\)</span></li>
<li>Prediction error when <span class="math inline">\(w_0 = 25,\!000\)</span> and <span class="math inline">\(w_1 = 300\)</span>: <span class="math inline">\(\left| 925,000 - 8000000 \right| = 125,000\)</span></li>
</ol>
<p>Given these results we might reasonably conclude that the second set of model weights produced the better prediction, since it was less wrong by $25,000.</p>
<p>In machine learning this prediction error is commonly called the model’s <em>loss,</em> that is, how far off the prediction is from the observed truth. The mathematical function by which we compute prediction error is called the <em>loss function</em>. In our case, we could represent our choice of loss function as <span class="math inline">\(\left| \hat{y} - y \right|\)</span>.</p>
<p>There are many possible choices of loss function. For example, we didn’t have to use absolute value, we could have simply taken the difference as our measurement of loss. We will discuss loss functions and introduce the most commonly used loss functions in later sections.</p>
</section>
<section id="training-data" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="training-data"><span class="header-section-number">3.2.2</span> Training Data</h3>
<p>In order for a computer to <em>learn</em> the best weight values for a model, we need to communicate with it in the language it understands: <strong>data</strong>. In this context, “data” refers to numerical values organized in rows and columns, like a spreadsheet or matrix. Each row is a single <strong>example</strong>, and each column holds a particular <strong>feature</strong> or <strong>label</strong>.</p>
<p>For our home value prediction example, imagine a simple dataset with two columns:</p>
<ul>
<li>One column for the <strong>square footage</strong> of a home (the <em>input</em>), and<br>
</li>
<li>One column for the <strong>value</strong> of the home (the <em>output</em>).</li>
</ul>
<p>Each row contains both the square footage and the corresponding value for a particular house. Together, each pair of values forms what we call an <strong>input-output pair</strong>. A complete set of these pairs is called a <strong>dataset</strong>.</p>
<p>We can write this dataset using the following compact mathematical notation:</p>
<p><span class="math display">\[
\{(x^{(i)}, y^{(i)})\}_{i=1}^n
\]</span></p>
<p>This might look intimidating at first, but it’s just a convenient way of saying:</p>
<blockquote class="blockquote">
<p>“We have <span class="math inline">\(n\)</span> examples. For each example <span class="math inline">\(i\)</span>, we observe an input <span class="math inline">\(x^{(i)}\)</span> and a corresponding output <span class="math inline">\(y^{(i)}\)</span>.”</p>
</blockquote>
<p>Let’s break down the notation a bit further:</p>
<ul>
<li>The curly braces <span class="math inline">\(\{ \}\)</span> mean we’re describing a set, a collection of items.</li>
<li>The superscript <span class="math inline">\((i)\)</span> just means “for example number <span class="math inline">\(i\)</span>”</li>
<li>Each item in the set is a pair: <span class="math inline">\((x^{(i)}, y^{(i)})\)</span> where:
<ul>
<li><span class="math inline">\(x^{(i)}\)</span> is the input (e.g., square footage) for the <span class="math inline">\(i\)</span>th example</li>
<li><span class="math inline">\(y^{(i)}\)</span> is the output (e.g., home value) for that same example</li>
</ul></li>
<li>The subscript <span class="math inline">\(_{i=1}^n\)</span> tells us there are <span class="math inline">\(n\)</span> examples and we are referring to all <span class="math inline">\(1\)</span> to <span class="math inline">\(n\)</span> of them.</li>
</ul>
<p>If you wrote this out as a table, it might look like this:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Example (<span class="math inline">\(i\)</span>)</th>
<th><span class="math inline">\(x^{(i)}\)</span> = Square Footage</th>
<th><span class="math inline">\(y^{(i)}\)</span> = Home Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1200</td>
<td>250,000</td>
</tr>
<tr class="even">
<td>2</td>
<td>1400</td>
<td>275,000</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1600</td>
<td>300,000</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n\)</span></td>
<td>(last example)</td>
<td></td>
</tr>
</tbody>
</table>
<p>This is the kind of data we use to “train” a machine learning model, by showing it many examples, we give it a chance to learn the relationship between inputs and outputs.</p>
<div class="callout callout-style-default callout-tip callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Terminology: Inputs and Outputs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In different fields and contexts, we often use different terms for the same underlying ideas. Here’s a helpful reference for the various names used for <strong>inputs</strong> and <strong>outputs</strong> in machine learning and related areas:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 33%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Common Synonyms</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input</strong></td>
<td>Feature, Independent Variable, Predictor, Covariate, Regressor, <span class="math inline">\(x\)</span></td>
<td>The value(s) we feed into the model to make a prediction. Can be one variable or many.</td>
</tr>
<tr class="even">
<td><strong>Output</strong></td>
<td>Label, Target, Dependent Variable, Response, <span class="math inline">\(y\)</span></td>
<td>The value the model is trying to predict or learn from.</td>
</tr>
<tr class="odd">
<td><strong>Input-Output Pair</strong></td>
<td>Example, Observation, Data Point, <span class="math inline">\((x^{(i)}, y^{(i)})\)</span></td>
<td>A single row of data showing both the input and the correct output.</td>
</tr>
<tr class="even">
<td><strong>Collection of Examples</strong></td>
<td>Dataset, Training Data, Sample</td>
<td>All the input-output pairs we give to the model to learn from.</td>
</tr>
<tr class="odd">
<td><strong>Predicted Output</strong></td>
<td>Prediction, Estimate, <span class="math inline">\(\hat{y}\)</span></td>
<td>The output the model thinks is correct, based on what it learned.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>There are actually four equivalent ways of representing a dataset, each useful in different contexts. You will be greatly aided in your study of machine learning if you can recognize and switch between all four forms with ease. Different textbooks, courses, tutorials and code libraries will use different representations so developing fluency in all of them will help you understand ideas more deeply and communicate more clearly.</p>
<ol type="1">
<li><strong>Mathematical Set Notation:</strong><br>
This is the notation we just introduced:<br>
<span class="math display">\[
\{(x^{(i)}, y^{(i)})\}_{i=1}^n
\]</span></li>
</ol>
<p>It represents the dataset as a collection of <span class="math inline">\(n\)</span> input-output pairs, where each <span class="math inline">\(x^{(i)}\)</span> is an input and each <span class="math inline">\(y^{(i)}\)</span> is the corresponding output. This form is widely used in textbooks and research papers because of its compactness and precision. It’s the language of mathematics, and it’s especially helpful when trying to understand what’s happening <em>under the hood</em> as computer code executes. By using this notation, we can reason more clearly about how models learn and make predictions.</p>
<ol start="2" type="1">
<li><p><strong>Spreadsheet or Table Format:</strong><br>
This is the most familiar form for most people and is often used in business and statistics. Each row represents an example; each column represents a variable or feature.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 36%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Example (<span class="math inline">\(i\)</span>)</th>
<th><span class="math inline">\(x^{(i)}\)</span> = Input (e.g., SqFt)</th>
<th><span class="math inline">\(y^{(i)}\)</span> = Output (e.g., Price)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1200</td>
<td>250,000</td>
</tr>
<tr class="even">
<td>2</td>
<td>1400</td>
<td>275,000</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1600</td>
<td>300,000</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n\)</span></td>
<td>—</td>
<td>—</td>
</tr>
</tbody>
</table>
<p>This is also the format used in tools like Excel, Google Sheets, or data frames in Python and R.</p></li>
<li><p><strong>Mathematical Matrix Notation:</strong><br>
Matrix notation is a compact and powerful mathematical way to represent an entire dataset, especially when trying to understand what calculations the computer is making while executing machine learning algorithms (discussed in the next section). In matrix form:</p></li>
</ol>
<ul>
<li>The <strong>inputs</strong> are stored in a matrix called <span class="math inline">\(X\)</span>, with <span class="math inline">\(n\)</span> rows (one for each example) and <span class="math inline">\(d\)</span> columns (one for each feature or input variable).</li>
<li>The <strong>outputs</strong> are stored in a column vector called <span class="math inline">\(y\)</span>, with <span class="math inline">\(n\)</span> rows (one for each label or true value).</li>
</ul>
<p>If each input has just one feature (like square footage), then <span class="math inline">\(X\)</span> is simply a single column of numbers (a vector). But if we have <strong>more than one input feature</strong>, for example, both square footage and number of bedrooms, then <span class="math inline">\(X\)</span> becomes a matrix with multiple columns, like this:</p>
<p><span class="math display">\[
X =
\begin{bmatrix}
x_1^{(1)} &amp; x_2^{(1)} \\
x_1^{(2)} &amp; x_2^{(2)} \\
\vdots &amp; \vdots \\
x_1^{(n)} &amp; x_2^{(n)}
\end{bmatrix}
\]</span></p>
<p>Here:</p>
<ul>
<li>Each <strong>row</strong> represents one house (one example from the dataset).</li>
<li>Each <strong>column</strong> represents a feature:
<ul>
<li><span class="math inline">\(x_1^{(i)}\)</span> might be the square footage of house <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(x_2^{(i)}\)</span> might be the number of bedrooms in house <span class="math inline">\(i\)</span></li>
</ul></li>
</ul>
<p>This structure easily extends to more features. For example, if we also include number of bathrooms, year built, and lot size, then <span class="math inline">\(X\)</span> would have 5 columns total (one for each feature).</p>
<p>The output vector <span class="math inline">\(y\)</span> stays the same regardless of the number of inputs with one value per row, like the price of the home:</p>
<p><span class="math display">\[
y =
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(n)}
\end{bmatrix}
\]</span></p>
<blockquote class="blockquote">
<p>Matrix notation lets us express model computations in a <strong>compact, efficient</strong> form that computers can execute quickly.</p>
<p>It will be essential when we <strong>scale up</strong> to more than input feature. It illustrates how a machine <em>thinks.</em> It’s ideal for: - Handling <strong>many examples and features</strong> - Performing <strong>vectorized calculations</strong> like dot products and gradients - Writing models in <strong>clean, code-friendly format</strong></p>
<p>Matrix notation makes your reasoning and implementation <strong>faster, clearer, and more powerful</strong>. We’ll use it throughout the course.</p>
</blockquote>
<ol start="4" type="1">
<li><p><strong>Code Representation (Arrays or Tensors):</strong><br>
When implementing models in code, we typically use arrays (Python’s NumPy package) or tensors (Python’s PyTorch package) which are data structures that store values in memory for numerical computation.</p>
<p>Below is an example with NumPy</p></li>
</ol>
<div id="117a7731" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># NumPy</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1200</span>], [<span class="dv">1400</span>], [<span class="dv">1600</span>]])</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([[<span class="dv">250000</span>], [<span class="dv">275000</span>], [<span class="dv">300000</span>]])</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"X ="</span>, X)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"y = "</span>, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X = [[1200]
 [1400]
 [1600]]
y =  [[250000]
 [275000]
 [300000]]</code></pre>
</div>
</div>
<p>So what does this mean?</p>
<ul>
<li><code>X</code> is a <strong>matrix of inputs</strong> (in this case, just one feature: square footage).<br>
Each <strong>row</strong> represents one example:
<ul>
<li>1200 square feet<br>
</li>
<li>1400 square feet<br>
</li>
<li>1600 square feet</li>
</ul></li>
<li><code>y</code> is a <strong>vector of outputs</strong> (the target values, like home prices).<br>
Each row in <code>y</code> matches the corresponding row in <code>X</code>.</li>
</ul>
<p>Let’s now connect this code back to the set and matrix representations of a dataset.</p>
<p><strong>Equivalence with Set Notation</strong></p>
<p>In set notation this tiny dataset could be represented as three input-output pairs:</p>
<p><span class="math display">\[
\{(x^{(i)}, y^{(i)})\}_{i=1}^3
\]</span> More specifically:</p>
<ul>
<li><span class="math inline">\(x^{(1)} = 1200\)</span>, <span class="math inline">\(y^{(1)} = 250{,}000\)</span><br>
</li>
<li><span class="math inline">\(x^{(2)} = 1400\)</span>, <span class="math inline">\(y^{(2)} = 275{,}000\)</span><br>
</li>
<li><span class="math inline">\(x^{(3)} = 1600\)</span>, <span class="math inline">\(y^{(3)} = 300{,}000\)</span></li>
</ul>
<p>Each pair <span class="math inline">\((x^{(i)}, y^{(i)})\)</span> is represented by a row in <code>X</code> and the corresponding row in <code>y</code>.</p>
<p><strong>Equivalence with Matrix Notation</strong></p>
<p>We can also see how this toy dataset would be written in matrix notation as a pair of matrices, one for the inputs and one for the outputs:</p>
<p><span class="math display">\[
X =
\begin{bmatrix}
x_1^{(1)} \\
x_1^{(2)} \\
x_1^{(3)}
\end{bmatrix}
=
\begin{bmatrix}
1200 \\
1400 \\
1600
\end{bmatrix},
\quad
y =
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
y^{(3)}
\end{bmatrix}
=
\begin{bmatrix}
250{,}000 \\
275{,}000 \\
300{,}000
\end{bmatrix}
\]</span></p>
<ul>
<li><span class="math inline">\(X\)</span> is an <span class="math inline">\(n \times 1\)</span> input matrix (with one feature per example),</li>
<li><span class="math inline">\(y\)</span> is an <span class="math inline">\(n \times 1\)</span> output vector (with one label per example).</li>
</ul>
<p>PyTorch Example</p>
<p>Lastly, below we represent the same toy dataset using <strong>PyTorch</strong>, currently the most popular deep learning library, and the one used to develop many modern large language models (LLMs).</p>
<p>PyTorch uses a data structure called a <strong>tensor</strong>.</p>
<p>A <strong>tensor</strong> is a generalization of familiar objects like scalars, vectors, and matrices:</p>
<ul>
<li>A <strong>scalar</strong> is a 0-dimensional tensor (e.g., <code>5</code>)</li>
<li>A <strong>vector</strong> is a 1-dimensional tensor (e.g., <code>[1200, 1400, 1600]</code>)</li>
<li>A <strong>matrix</strong> is a 2-dimensional tensor (e.g., a table of numbers)</li>
<li>A <strong>3D tensor</strong> is a collection or stack of matrices. You can think of it as a book of matrices, where each page is a 2D grid (matrix), and the whole book has depth (i.e., the number of pages)</li>
<li>A <strong>4D tensor</strong> is a collection of 3D tensors. You can think of it as a <strong>library of books</strong>, where:
<ul>
<li>Each <strong>book</strong> is a 3D tensor (a stack of matrices),</li>
<li>Each <strong>page</strong> in a book is a matrix,</li>
<li>And each matrix contains rows and columns of numbers.</li>
</ul></li>
</ul>
<p>Increasing the tensor dimension allows us to compactly describe multiple sets of structured data and for a computer to perform parallel computations efficiently which is essential when training modern LLMs.</p>
<p>In theory, there is no limit on the number of dimensions a tensor can have.</p>
<p>In practice, we won’t need more than 4D tensors to build modern large language models (LLMs).</p>
<p>For simple models like linear regression, a 2D tensor (i.e., a matrix) is sufficient to represent the data.</p>
<p>Below we illustrate the same toy data set using a PyTorch tensor.</p>
<div id="855a9ba1" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Input data: square footage (in one column)</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.tensor([[<span class="fl">1200.</span>], [<span class="fl">1400.</span>], [<span class="fl">1600.</span>]])</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Output data: home prices</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([[<span class="fl">250000.</span>], [<span class="fl">275000.</span>], [<span class="fl">300000.</span>]])</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"X (inputs):"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">y (outputs):"</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X (inputs):
tensor([[1200.],
        [1400.],
        [1600.]])

y (outputs):
tensor([[250000.],
        [275000.],
        [300000.]])</code></pre>
</div>
</div>
<p>The dot (.) at the end of the numbers (like 1200. or 250000.) indicates that the numbers are being treated as floating point numbers (i.e., float type) rather than integers. A floating point number is a number that can represent decimal values on a computer, in contrast to an integer, which can only represent whole numbers. PyTorch models expect inputs and outputs to be floating point numbers because most model operations involve decimals.</p>
</section>
<section id="loss-function" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="loss-function"><span class="header-section-number">3.2.3</span> Loss Function</h3>
<p>Let’s now plot a larger dataset and visualize the line of best fit</p>
<div id="bab9ef05" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Set Seed for Reproducibility ---</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Randomly Generate Data ---</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>X_data <span class="op">=</span> np.random.uniform(<span class="dv">1200</span>, <span class="dv">3000</span>, size<span class="op">=</span>(n_samples, <span class="dv">1</span>)).astype(np.float32)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>true_slope <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>true_intercept <span class="op">=</span> <span class="dv">50000</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">30000</span>, size<span class="op">=</span>(n_samples, <span class="dv">1</span>)).astype(np.float32)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>y_data <span class="op">=</span> true_intercept <span class="op">+</span> true_slope <span class="op">*</span> X_data <span class="op">+</span> noise</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># --- scikit-learn Model ---</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X_data)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>lr_model <span class="op">=</span> LinearRegression()</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>lr_model.fit(X_scaled, y_data)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert scikit-learn model back to original units</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>slope_sk <span class="op">=</span> lr_model.coef_[<span class="dv">0</span>][<span class="dv">0</span>] <span class="op">/</span> scaler.scale_[<span class="dv">0</span>]</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>intercept_sk <span class="op">=</span> lr_model.intercept_[<span class="dv">0</span>] <span class="op">-</span> slope_sk <span class="op">*</span> scaler.mean_[<span class="dv">0</span>]</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="co"># --- PyTorch Model ---</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>X_torch <span class="op">=</span> torch.tensor(X_data)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>y_torch <span class="op">=</span> torch.tensor(y_data)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>X_mean <span class="op">=</span> X_torch.mean()</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>X_std <span class="op">=</span> X_torch.std(unbiased<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>X_scaled_torch <span class="op">=</span> (X_torch <span class="op">-</span> X_mean) <span class="op">/</span> X_std</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3000</span>):</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model(X_scaled_torch)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y_pred, y_torch)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert PyTorch model back to original units</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>slope_torch <span class="op">=</span> model.weight.item() <span class="op">/</span> X_std.item()</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>intercept_torch <span class="op">=</span> model.bias.item() <span class="op">-</span> slope_torch <span class="op">*</span> X_mean.item()</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Plot ---</span></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>X_plot <span class="op">=</span> np.linspace(<span class="dv">1100</span>, <span class="dv">3100</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>y_sk <span class="op">=</span> intercept_sk <span class="op">+</span> slope_sk <span class="op">*</span> X_plot</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>y_pt <span class="op">=</span> intercept_torch <span class="op">+</span> slope_torch <span class="op">*</span> X_plot</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot scikit-learn</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(X_data, y_data, label<span class="op">=</span><span class="st">"Actual Data"</span>, color<span class="op">=</span><span class="st">"blue"</span>)</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(X_plot, y_sk, label<span class="op">=</span><span class="st">"scikit-learn Fit"</span>, color<span class="op">=</span><span class="st">"green"</span>)</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"scikit-learn Regression"</span>)</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">"Square Footage"</span>)</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">"Home Value"</span>)</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>)</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot PyTorch</span></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(X_data, y_data, label<span class="op">=</span><span class="st">"Actual Data"</span>, color<span class="op">=</span><span class="st">"blue"</span>)</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(X_plot, y_pt, label<span class="op">=</span><span class="st">"PyTorch Fit"</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"PyTorch Regression"</span>)</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">"Square Footage"</span>)</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>)</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01-machine-learning_files/figure-html/cell-4-output-1.png" width="1333" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="learning-weight-values" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="learning-weight-values"><span class="header-section-number">3.2.4</span> Learning weight values</h3>
<p>In the example above, we assumed <span class="math inline">\(w_0 = 50,000\)</span> and <span class="math inline">\(w_1 = 200\)</span> for illustation. We now turn attention to how a machine (i.e., a computer) is capable of <em>learning</em> the optimal weight values, enabling the mathematical model to make the most accurate predictions possible which explains which strikes at the heart of why the field is called called <em>machine learning</em>.</p>
<p>There are four main ingredients necessary for the computer to <em>learn</em> the best possible model weights:</p>
<ol type="1">
<li><p><strong>A proposed mathematical structure</strong>: For simple linear regression the mathematical structure (i.e., equation) is <span class="math inline">\(y = w_0 + w_1 x\)</span>. This equation defines the form of the relationship between inputs(s) and output(s). This structure assumes a linear relationship between the input <span class="math inline">\(x\)</span> and output <span class="math inline">\(y\)</span>. Later in the course, we will explore the more complex non-linear models called neural networks which are used in large language models.</p></li>
<li><p><strong>Training Data</strong> —</p></li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation: Superscripts vs.&nbsp;Subscripts
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If you’re coming from econometrics, you’re probably used to indexing observations using <strong>subscripts</strong>, like <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>. In machine learning, we typically use <strong>superscripts</strong>, like <span class="math inline">\(x^{(i)}\)</span> and <span class="math inline">\(y^{(i)}\)</span>, to denote the <span class="math inline">\(i\)</span>-th training example.</p>
<p>Why the change?</p>
<p>Machine learning models often work with <strong>vectors</strong> of features where <span class="math inline">\(x\)</span> is understood not as single input variable but as a vector of input variables:</p>
<p><span class="math display">\[
x^{(i)} = \begin{bmatrix} x_1^{(i)} \\ x_2^{(i)} \\ \vdots \\ x_d^{(i)} \end{bmatrix}
\]</span></p>
<p>Here:</p>
<ul>
<li>For simple linear regression there is only a single input variable so <span class="math inline">\(x=x_1\)</span>. However, say we added <em>year built</em> as a second input variable we could represent this as <span class="math inline">\(x_2\)</span>.</li>
<li>The <strong>superscript</strong> <span class="math inline">\((i)\)</span> indicates the <span class="math inline">\(i\)</span>-th training example.</li>
<li>The <strong>subscript</strong> <span class="math inline">\(j\)</span> (e.g., <span class="math inline">\(x_j^{(i)}\)</span>) refers to the <span class="math inline">\(j\)</span>-th feature of that example.</li>
</ul>
<p>Using subscripts for both observations and features (e.g., <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>) would lead to confusion.</p>
<p>So while this might feel unfamiliar at first, <strong>superscript indexing</strong> is standard in ML to keep feature and example indexing clearly separated.</p>
</div>
</div>
</div>
<ol start="3" type="1">
<li><strong>A loss function</strong> — A <em>loss function</em> is another mathematical equation whose purpose it to quantify the error between the model’s predictions <span class="math inline">\(\hat{y}^{(i)}\)</span> and the true output values observed in the training data <span class="math inline">\(y^{(i)}\)</span>. For regression, the typical choice of loss function is <strong>mean squared error</strong> (MSE) defined as:</li>
</ol>
<p><span class="math display">\[
\mathcal{L}(w_0, w_1) = \frac{1}{n} \sum_{i=1}^n (w_0 + w_1 x^{(i)} - y^{(i)})^2 = \frac{1}{n} \sum_{i=1}^n (\hat{y}^{(i)} - y^{(i)})^2
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Unpacking the notation of the MSE Loss Function
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This is called the Mean Squared Error (MSE) loss function, and it helps us measure how good or bad our model’s predictions are.</p>
<p>The big curly <span class="math inline">\(\mathcal{L}\)</span> is a stylized version of the capital Latin letter L. In this context, <span class="math inline">\(\mathcal{L}\)</span> stands for “Loss” or sometimes “Loss function.”</p>
<ul>
<li><span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> are the parameters (or weights) of our simple linear regression model. Notice that they are playing the roles of unknown variables in this notation: <span class="math inline">\(\mathcal{L}(w_0, w_1)\)</span>. This is because we don’t know what these values are or should be yet, that’s what we want the computer <em>learn</em> for us.</li>
<li><span class="math inline">\(x^{(i)}\)</span> is the input (e.g., square footage of a house)</li>
<li><span class="math inline">\(\hat{y}^{(i)} = w_0 + w_1 x^{(i)}\)</span> is the model’s predicted output (e.g., the predicted house price).</li>
<li><span class="math inline">\(y^{(i)}\)</span> is the true value (the actual house price for data point <span class="math inline">\(i\)</span>).</li>
<li><span class="math inline">\((\hat{y}^{(i)} - y^{(i)})^2\)</span> is the squared error: how far off the prediction is, squared to make sure it’s always positive and to penalize big mistakes more than small ones.</li>
<li><span class="math inline">\(\sum_{i=1}^n\)</span> adds up the squared errors for all <span class="math inline">\(n\)</span> data points in our dataset (the Greek letter <span class="math inline">\(\sum\)</span> is called “capital sigma” and means we should “add up a bunch of terms”)</li>
<li><span class="math inline">\(\frac{1}{n}\)</span> takes the average of all those squared error terms.</li>
</ul>
<p>This function tells us how well our model is performing overall. A smaller value means our predictions on average are close to the true values, and a larger value means on average we’re making bigger mistakes.</p>
<p>Our goal is to find the values of <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> that make this loss function as small as possible, with for simpple linear regression equates to finding the line of best-fit through our data.</p>
<ol start="4" type="1">
<li><strong>Algorithm(s)</strong> — The final ingredient(s) we need are algorithms. An algorithm is a step-by-step, well-defined procedure for performing a task. For example, long division is an algorithm wherein you follow a well defined procedure to compute a mathematical operation that would be difficult to do mentally. In this course, the task we are performing is <em>learning</em> the optimal values for model weights. Hence, for our purposes, we will study the <strong>gradient descent</strong> algorithm which iteratively adjust weights to minimize the loss function.</li>
</ol>
<p>Understanding these mappings will help you bridge the language used in machine learning and econometrics and navigate both literatures more fluently.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why are we calling this “supervised” machine learning and what other types are there?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">

</div>
</div>
</div>
<p>In our example, <strong>training data</strong>,</p>
<p><strong>machine learning algorithm</strong></p>
<p><strong>parameters or “weights”</strong></p>
<p>(read as y-hat, the “hat” signifies that a prediction is being made. Writing <span class="math inline">\(y\)</span> without the “hat” would mean no prediction has yet been attempted).</p>
<p>uch as a zestimate from Zillow</p>
</section>
</section>
<section id="glossary" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="glossary"><span class="header-section-number">3.3</span> Glossary</h2>
<p><strong>Simple Linear Regression</strong></p>
<p><strong>parameters or “weights”</strong></p>
<p><strong>training data</strong>,</p>
<p><strong>machine learning algorithm</strong></p>
<p><strong>inference</strong></p>
<p><strong>supervised machine learning</strong></p>
<p><strong>Epoch</strong></p>
<ul>
<li><span class="math inline">\(w_0\)</span> is the intercept (bias term),</li>
<li><span class="math inline">\(w_1\)</span> is the slope (weight).</li>
</ul>
<p>To bring this to life, let’s consider a concrete where we’d like to predict the price of a home.</p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(\hat{y}\)</span> be the predicted price of a house (in thousands of dollars),</li>
<li><span class="math inline">\(x\)</span> be the size of the house in square feet.</li>
</ul>
<p>Suppose we have fit a linear model to a dataset and obtained:</p>
<p><span class="math display">\[
\hat{y} = 50 + 0.2 x
\]</span></p>
<p>This means: - The base price of any house (intercept) is $50,000. - For each additional square foot, the price increases by $200.</p>
<section id="example" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="example"><span class="header-section-number">3.3.1</span> Example</h3>
<p>For a house that is 1,000 square feet:</p>
<p><span class="math display">\[
\hat{y} = 50 + 0.2 \cdot 1000 = 250
\]</span></p>
<p>So, the model predicts a price of $250,000 for a 1,000 square foot house.</p>
<hr>
<section id="target-variable-and-input-variable" class="level4" data-number="3.3.1.1">
<h4 data-number="3.3.1.1" class="anchored" data-anchor-id="target-variable-and-input-variable"><span class="header-section-number">3.3.1.1</span> Target Variable and Input Variable</h4>
<ul>
<li><strong>Target Variable</strong> (<span class="math inline">\(y\)</span>): the outcome we are trying to predict.</li>
<li><strong>Input Variable</strong> (<span class="math inline">\(x\)</span>): the feature used to make the prediction.</li>
</ul>
<hr>
</section>
<section id="training-and-validation-sets" class="level4" data-number="3.3.1.2">
<h4 data-number="3.3.1.2" class="anchored" data-anchor-id="training-and-validation-sets"><span class="header-section-number">3.3.1.2</span> Training and Validation Sets</h4>
<ul>
<li><strong>Training set</strong>: used to learn the model parameters.</li>
<li><strong>Validation set</strong>: used to evaluate how well the model generalizes to unseen data.</li>
</ul>
<hr>
</section>
<section id="model-approximation-and-noise" class="level4" data-number="3.3.1.3">
<h4 data-number="3.3.1.3" class="anchored" data-anchor-id="model-approximation-and-noise"><span class="header-section-number">3.3.1.3</span> Model Approximation and Noise</h4>
<p>The model approximates an unknown true function: <span class="math display">\[
y = f^*(x) + \varepsilon
\]</span> where <span class="math inline">\(\varepsilon\)</span> is the irreducible error.</p>
<hr>
</section>
<section id="residuals" class="level4" data-number="3.3.1.4">
<h4 data-number="3.3.1.4" class="anchored" data-anchor-id="residuals"><span class="header-section-number">3.3.1.4</span> Residuals</h4>
<p>The residuals measure the difference between actual and predicted values: <span class="math display">\[
e_i = y_i - \hat{y}_i
\]</span></p>
<hr>
</section>
<section id="loss-function-mean-squared-error" class="level4" data-number="3.3.1.5">
<h4 data-number="3.3.1.5" class="anchored" data-anchor-id="loss-function-mean-squared-error"><span class="header-section-number">3.3.1.5</span> Loss Function (Mean Squared Error)</h4>
<p>To measure model performance: <span class="math display">\[
\mathcal{L}(w_0, w_1) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\]</span></p>
<hr>
</section>
<section id="design-matrix" class="level4" data-number="3.3.1.6">
<h4 data-number="3.3.1.6" class="anchored" data-anchor-id="design-matrix"><span class="header-section-number">3.3.1.6</span> Design Matrix</h4>
<p>With <span class="math inline">\(n\)</span> observations, the design matrix includes a bias term: <span class="math display">\[
\mathbf{X} = \begin{bmatrix}
1 &amp; x_1 \\
1 &amp; x_2 \\
\vdots &amp; \vdots \\
1 &amp; x_n
\end{bmatrix}
\]</span></p>
<hr>
</section>
<section id="closed-form-solution-normal-equation" class="level4" data-number="3.3.1.7">
<h4 data-number="3.3.1.7" class="anchored" data-anchor-id="closed-form-solution-normal-equation"><span class="header-section-number">3.3.1.7</span> Closed-form Solution (Normal Equation)</h4>
<p>We can directly compute the optimal weights: <span class="math display">\[
\mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
\]</span></p>
<hr>
</section>
<section id="parameters-and-estimation" class="level4" data-number="3.3.1.8">
<h4 data-number="3.3.1.8" class="anchored" data-anchor-id="parameters-and-estimation"><span class="header-section-number">3.3.1.8</span> Parameters and Estimation</h4>
<ul>
<li><strong>Parameters</strong>: <span class="math inline">\(w_0\)</span>, <span class="math inline">\(w_1\)</span></li>
<li><strong>Goal</strong>: estimate parameters that minimize the loss function</li>
</ul>
<hr>
</section>
<section id="gradient-descent" class="level4" data-number="3.3.1.9">
<h4 data-number="3.3.1.9" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">3.3.1.9</span> Gradient Descent</h4>
<p>When a closed-form is not used, we apply an iterative optimization: <span class="math display">\[
w_j \leftarrow w_j - \eta \frac{\partial \mathcal{L}}{\partial w_j}
\]</span></p>
<p>With gradients: <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial w_j} = -\frac{2}{n} \sum_{i=1}^n x_{ij}(y_i - \hat{y}_i)
\]</span></p>
<hr>
</section>
<section id="overfitting-and-underfitting" class="level4" data-number="3.3.1.10">
<h4 data-number="3.3.1.10" class="anchored" data-anchor-id="overfitting-and-underfitting"><span class="header-section-number">3.3.1.10</span> Overfitting and Underfitting</h4>
<ul>
<li><strong>Underfitting</strong>: model too simple to capture data patterns</li>
<li><strong>Overfitting</strong>: model too complex, captures noise instead of signal</li>
<li><strong>Validation</strong> helps detect these behaviors.</li>
</ul>
</section>
</section>
<section id="multiple-linear-regression" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="multiple-linear-regression"><span class="header-section-number">3.3.2</span> Multiple Linear Regression</h3>
</section>
<section id="logistic-regression" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">3.3.3</span> Logistic Regression</h3>
</section>
<section id="classification" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="classification"><span class="header-section-number">3.3.4</span> Classification</h3>
</section>
</section>
<section id="loss-functions" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="loss-functions"><span class="header-section-number">3.4</span> Loss Functions</h2>
<p>A loss function is a mathematical function that measures the difference between a model’s predicted output and the true target value from a training dataset. It produces a scalar value representing the error for a single data point or the average error over a batch.</p>
<ul>
<li><p><strong>Cross entropy loss</strong>: “cross entropy” and “negative average log probability” related and often used interchangeably in practice.”</p></li>
<li><p><strong>Perplexity</strong>: “Perplexity is a measure often used alongside cross entropy loss to evaluate the performance of models in tasks like language modeling.”</p></li>
</ul>
</section>
<section id="gradient-descent-1" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="gradient-descent-1"><span class="header-section-number">3.5</span> Gradient Descent</h2>
<div class="callout callout-style-default callout-tip callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Synonyms Across Fields
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Different disciplines often use different terms for the same concepts. Here’s a helpful guide:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 30%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Machine Learning</th>
<th>Statistics / Econometrics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Single data point (e.g., a row in a spreadsheet)</td>
<td><strong>Training example</strong></td>
<td><strong>Observation</strong>, <strong>Case</strong></td>
</tr>
<tr class="even">
<td>Input features</td>
<td><strong>Features</strong>, <strong>Inputs</strong></td>
<td><strong>Independent variables</strong>, <strong>Regressors</strong>, <strong>Covariates</strong>, <strong>Predictors</strong></td>
</tr>
<tr class="odd">
<td>Target output</td>
<td><strong>Label</strong>, <strong>Target</strong>, <strong>Output</strong></td>
<td><strong>Dependent variable</strong>, <strong>Response</strong></td>
</tr>
<tr class="even">
<td>Collection of data points</td>
<td><strong>Training set</strong>, <strong>Training data</strong></td>
<td><strong>Sample</strong>, <strong>Dataset</strong></td>
</tr>
<tr class="odd">
<td>Parameter</td>
<td><strong>Weight</strong> (e.g., <span class="math inline">\(w_1\)</span>)</td>
<td><strong>Coefficient</strong> (e.g., <span class="math inline">\(\beta_1\)</span>)</td>
</tr>
<tr class="even">
<td>Model prediction</td>
<td><span class="math inline">\(\hat{y}\)</span></td>
<td><strong>Fitted value</strong>, <strong>Predicted value</strong></td>
</tr>
<tr class="odd">
<td>Loss function</td>
<td><strong>Loss</strong></td>
<td><strong>Objective function</strong>, <strong>Criterion function</strong></td>
</tr>
<tr class="even">
<td>Learn model parameters</td>
<td><strong>Training</strong>, <strong>Fitting</strong></td>
<td><strong>Estimation</strong></td>
</tr>
</tbody>
</table>
<p>Understanding these parallels can make it much easier to bridge terminology between fields as you move through the course.</p>
</div>
</div>
</div>
<section id="supervised-learning-use-cases" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="supervised-learning-use-cases"><span class="header-section-number">3.5.1</span> Supervised Learning Use Cases</h3>
<p><strong>Supervised Learning (Regression) Use Cases</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Use Case</th>
<th>Sample Inputs</th>
<th>Model Output Description</th>
<th>What ML Question is Being Answered?</th>
<th>What Business Question is Being Answered?</th>
<th>Example Algorithm(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Price Prediction</td>
<td>Property size, location, features</td>
<td>Numeric price</td>
<td>What is the expected price of this item?</td>
<td>How should I price products to stay competitive and profitable?</td>
<td>Linear Regression, CatBoost</td>
</tr>
<tr class="even">
<td>Demand Forecasting</td>
<td>Historical sales, promotions, holidays</td>
<td>Predicted sales volume</td>
<td>What will sales be next week/month?</td>
<td>How can I manage inventory or staffing to meet demand?</td>
<td>Linear Regression, LSTM</td>
</tr>
<tr class="odd">
<td>Medical Risk Score</td>
<td>Patient vitals, history</td>
<td>Risk score (e.g., probability of event)</td>
<td>How likely is a medical event to occur?</td>
<td>How should I prioritize preventive care for patients?</td>
<td>Random Forest, Ridge Reg.</td>
</tr>
<tr class="even">
<td>Revenue Forecasting</td>
<td>Past financials, seasonality</td>
<td>Revenue over next time period</td>
<td>How much revenue will we make?</td>
<td>How can I allocate budgets or set growth targets?</td>
<td>Time Series Models, XGBoost</td>
</tr>
<tr class="odd">
<td>Energy Usage Estimation</td>
<td>Time of day, weather, appliance use</td>
<td>Predicted energy consumption</td>
<td>How much energy will be used in this period?</td>
<td>How should I manage power supply or optimize grid efficiency?</td>
<td>Linear Regression, SVR</td>
</tr>
</tbody>
</table>
<p><strong>Supervised Learning (Classification) Use Cases</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Use Case</th>
<th>Sample Inputs</th>
<th>Model Output Description</th>
<th>What ML Question is Being Answered?</th>
<th>What Business Question is Being Answered?</th>
<th>Example Algorithm(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Email Spam Detection</td>
<td>Email content, sender info, subject line</td>
<td>Binary label: spam or not spam</td>
<td>Is this email spam?</td>
<td>How can I prevent unwanted emails from reaching users’ inboxes?</td>
<td>Logistic Regression, SVM</td>
</tr>
<tr class="even">
<td>Credit Risk Scoring</td>
<td>Income, credit history, employment data</td>
<td>Risk category (e.g., low/medium/high)</td>
<td>Will this applicant default?</td>
<td>Should I approve this loan, and at what interest rate?</td>
<td>Decision Tree, XGBoost</td>
</tr>
<tr class="odd">
<td>Image Classification</td>
<td>Pixel values from an image</td>
<td>Object class label (e.g., “cat”, “dog”)</td>
<td>What object is in this image?</td>
<td>How can I organize photos or automate product tagging?</td>
<td>CNNs, ResNet</td>
</tr>
<tr class="even">
<td>Sentiment Analysis</td>
<td>Review text, social media posts</td>
<td>Sentiment label (positive/negative)</td>
<td>What sentiment is being expressed?</td>
<td>What is the public opinion about my product or brand?</td>
<td>Naive Bayes, BERT</td>
</tr>
<tr class="odd">
<td>Disease Diagnosis</td>
<td>Symptoms, test results, demographics</td>
<td>Disease class (e.g., flu, COVID, none)</td>
<td>What condition does this patient likely have?</td>
<td>How can I assist doctors in making accurate and timely diagnoses?</td>
<td>Random Forest, Neural Nets</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="unsupervised-learning" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="unsupervised-learning"><span class="header-section-number">3.6</span> Unsupervised Learning</h2>
<section id="unsupervised-learning-use-cases" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="unsupervised-learning-use-cases"><span class="header-section-number">3.6.1</span> Unsupervised Learning Use Cases</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Use Case</th>
<th>Sample Inputs</th>
<th>Model Output Description</th>
<th>What ML Question is Being Answered?</th>
<th>What Business Question is Being Answered?</th>
<th>Example Algorithm(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Customer Segmentation</td>
<td>Age, income, purchase history</td>
<td>Cluster/group labels for each customer</td>
<td>What types of customers exist in my data?</td>
<td>How can I tailor marketing strategies to different customer types?</td>
<td>K-means, DBSCAN</td>
</tr>
<tr class="even">
<td>Topic Modeling</td>
<td>Articles or documents</td>
<td>Topics with keywords per document</td>
<td>What topics are being discussed?</td>
<td>What content themes resonate most with my audience or market?</td>
<td>LDA, NMF</td>
</tr>
<tr class="odd">
<td>Anomaly Detection</td>
<td>Transaction logs, sensor data</td>
<td>Anomaly score or binary flag</td>
<td>Which data points are unusual?</td>
<td>Are there fraudulent transactions or system failures I need to act on?</td>
<td>Isolation Forest, Autoencoder</td>
</tr>
<tr class="even">
<td>Dimensionality Reduction</td>
<td>High-dimensional features (e.g., pixels)</td>
<td>2D or 3D projections for analysis or visualization</td>
<td>How can I reduce feature space while preserving info?</td>
<td>How can I visualize or simplify complex data for human analysis or modeling?</td>
<td>PCA, t-SNE, UMAP</td>
</tr>
<tr class="odd">
<td>Market Basket Analysis</td>
<td>Sets of purchased items</td>
<td>Association rules (A &amp; B → C)</td>
<td>What items co-occur frequently in purchases?</td>
<td>Which product bundles or cross-sell offers should I promote?</td>
<td>Apriori, FP-Growth</td>
</tr>
<tr class="even">
<td>Word Embedding</td>
<td>Text corpus</td>
<td>Word vectors capturing semantic similarity</td>
<td>What are the contextual relationships between words?</td>
<td>How can I build a smarter search engine or chatbot that understands language context?</td>
<td>Word2Vec, GloVe</td>
</tr>
<tr class="odd">
<td>Image Compression</td>
<td>Raw pixel arrays</td>
<td>Compressed version of the image</td>
<td>How can I represent this image with fewer features?</td>
<td>How can I reduce storage or transmission costs for image data?</td>
<td>Autoencoders</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="reinforcement-learning" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="reinforcement-learning"><span class="header-section-number">3.7</span> Reinforcement Learning</h2>
<section id="reinforcement-learning-use-cases" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="reinforcement-learning-use-cases"><span class="header-section-number">3.7.1</span> Reinforcement Learning Use Cases</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Use Case</th>
<th>Sample Inputs</th>
<th>Model Output Description</th>
<th>What ML Question is Being Answered?</th>
<th>What Business Question is Being Answered?</th>
<th>Example Algorithm(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Game Playing</td>
<td>Game state (e.g., board, score)</td>
<td>Action to take</td>
<td>What should I do to win the game?</td>
<td>How can I build an AI that outperforms humans or creates adaptive gameplay?</td>
<td>Q-learning, DQN</td>
</tr>
<tr class="even">
<td>Robotics &amp; Control</td>
<td>Sensor data (angles, velocities, etc.)</td>
<td>Movement or control signals</td>
<td>How should the agent move next to reach a goal?</td>
<td>How can I automate physical tasks like picking, sorting, or navigating?</td>
<td>PPO, SAC, DDPG</td>
</tr>
<tr class="odd">
<td>Autonomous Vehicles</td>
<td>Sensor input (camera, LIDAR, speed, GPS)</td>
<td>Driving action</td>
<td>What’s the optimal next driving move?</td>
<td>How can I develop a safe and efficient self-driving vehicle system?</td>
<td>Deep RL + sensor fusion</td>
</tr>
<tr class="even">
<td>Recommendation Systems</td>
<td>User history, preferences, session behavior</td>
<td>Recommended item</td>
<td>What should I recommend next?</td>
<td>How can I increase user retention, engagement, or sales?</td>
<td>Contextual Bandits, RL</td>
</tr>
<tr class="odd">
<td>Portfolio Management</td>
<td>Financial indicators, stock prices</td>
<td>Asset allocation decision</td>
<td>How should I invest to maximize return?</td>
<td>How can I build an automated trading or portfolio optimization system?</td>
<td>Actor-Critic methods</td>
</tr>
<tr class="even">
<td>Personalized Education</td>
<td>Student progress and quiz results</td>
<td>Next learning step</td>
<td>What lesson or content should come next?</td>
<td>How can I boost student outcomes by personalizing learning pathways?</td>
<td>Multi-armed bandits</td>
</tr>
<tr class="odd">
<td>Healthcare Treatment</td>
<td>Patient history and vitals</td>
<td>Treatment or intervention strategy</td>
<td>What care plan maximizes long-term patient health?</td>
<td>How can I optimize healthcare outcomes while reducing costs and readmissions?</td>
<td>Off-policy RL, POMDPs</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="machine-learning" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="machine-learning"><span class="header-section-number">3.8</span> Machine Learning</h2>
<p>Supervised Learning: “Here’s the question and the right answer.”<br>
Unsupervised Learning: “Here is some data, can you find patterns and/or organize it in a meaningful way?”<br>
Reinforcement Learning: “You’re an agent playing a, figure out the best strategy through trial and error, using only points (rewards) as feedback.”</p>
<p>“During training, a machine learning algorithm processes a dataset and chooses the function that best matches the patterns in the data.”</p>
<p>supervised, unsupervised, and reinforcement.</p>
</section>
<section id="supervised-learning" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="supervised-learning"><span class="header-section-number">3.9</span> Supervised Learning</h2>
<p>Supervised learning is the most widely used type of machine learning and significantly overlaps with the methodologies of other quantitative fields, such as statistics and econometrics.</p>
<p>Supervised learning gets its name from the fact that the input data set has examples of both inputs and outputs.</p>
<p>The outputs are called labels, hence you will sometime hear the term “labeled dataset.”</p>
<ul>
<li><p><strong>logistic (sigmoid) function</strong>:</p></li>
<li><p><strong>Softmax</strong>:</p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./00-math-review.html" class="pagination-link" aria-label="Math and Python Review">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Math and Python Review</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02-deep-learning.html" class="pagination-link" aria-label="Deep Learning">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Deep Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>