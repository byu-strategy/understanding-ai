[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "",
    "text": "1 Syllabus\nFALL 2025\nInstructor: Scott Murff\nTAs: TBD Class Time: M/W, 3:30–4:45 PM\nClass Location: TNRB W308\nLMS: LearningSuite",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "index.html#content-overview",
    "href": "index.html#content-overview",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "1.1 Content Overview",
    "text": "1.1 Content Overview\nWelcome to Understanding AI: From Foundations to Strategy. In this course, students will learn where large language models (LLMs) like ChatGPT fit into the broader landscape of Artificial Intelligence (AI). Students will then build an LLM from the ground up using Python. The course concludes with a strategic exploration of how AI is transforming companies and society and how to harness it for good.\nThe course will begin with an essential math review but to excel in the course students should have some prior experience with differential calculus (i.e. derivatives), matrix multiplication (i.e. multiplying two or more matrices together), conditional probability, and basic Python programming.\n\n\n\n\n\n\n\n\nImage source: Build a Large Language Model (From Scratch) by Sebastian Raschka",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "index.html#who-should-take-this-course",
    "href": "index.html#who-should-take-this-course",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "1.2 Who Should Take This Course",
    "text": "1.2 Who Should Take This Course\nThis course is intended for advanced undergraduates and/or masters students who want a deep, technical introduction to LLMs alongside strategic insight into their real-world deployment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "1.3 Learning Outcomes",
    "text": "1.3 Learning Outcomes\n\n\n\n\n\n\n\nLearning Outcome\nTarget BYU Aim(s)\n\n\n\n\n1. Students will be able to implement basic AI and machine learning algorithms from first principles, including linear regression and neural networks, and apply these to Generative AI and LLM use cases.\nIntellectually Enlarging\n\n\n2. Students will analyze and explain the ethical, spiritual, and social implications of AI technologies, including their alignment (or misalignment) with eternal truths and principles of discipleship.\nSpiritually Strengthening, Character Building\n\n\n3. Students will demonstrate Christlike attributes such as diligence, humility, and purposeful curiosity in collaborative learning and problem-solving related to AI topics.\nCharacter Building\n\n\n4. Students will articulate how their understanding of AI can be used to bless the lives of others and reflect their discipleship to Jesus Christ in the workplace and community.\nLifelong Learning and Service, Spiritually Strengthening",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "index.html#instructor-bio",
    "href": "index.html#instructor-bio",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "1.4 Instructor Bio",
    "text": "1.4 Instructor Bio\nScott Murff is an Associate Teaching Professor of Strategy at the BYU Marriott School of Business, where he also serves as program director and teaches courses on business strategy, decision-making, and artificial intelligence. He brings over 15 years of experience at the intersection of business and technology, having worked as a consultant, product manager, and data scientist.\nPrior to joining BYU, Scott spent nearly seven years at McKinsey & Company in roles ranging from analytics specialist consultant to principal product manager, where he led product development and performance management initiatives for Fortune 500 clients. His earlier career includes building forecasting models as a VP at Zions Bancorporation and conducting regulatory research at the U.S. Office of the Comptroller of the Currency.\nScott holds a Master’s degree in Management Science & Engineering from Stanford University and a B.A. in Economics from BYU. He is passionate about helping students apply AI, analytics, and strategy to meaningful real-world problems with both rigor and purpose.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "1.5 Schedule",
    "text": "1.5 Schedule\nComing Soon.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "1.6 Grading",
    "text": "1.6 Grading\nThe course is not graded on a curve. It’s possible for every student to earn an A. However, achieving an A is challenging and demonstrates true excellence. The grading scale is show below:\n\n\n\nLetter Grade\nPercentage Range\nGPA\n\n\n\n\nA\n93–100%\n4.0\n\n\nA-\n90–92%\n3.7\n\n\nB+\n87–89%\n3.4\n\n\nB\n83–86%\n3.0\n\n\nB-\n80–82%\n2.7\n\n\nC+\n77–79%\n2.4\n\n\nC\n73–76%\n2.0\n\n\nC-\n70–72%\n1.7\n\n\nD+\n67–69%\n1.4\n\n\nD\n63–66%\n1.0\n\n\nD-\n60–62%\n0.7\n\n\nE (Fail)\nBelow 60%\n0.0\n\n\n\n\n1.6.1 Late work policy\nLate work will be accepted up to 9 days late for partial credit with a 10% penalty per day according to the following schedule\n\n\n\nDays Late\nLate Penalty\n\n\n\n\n1\n-10%\n\n\n2\n-20%\n\n\n3\n-30%\n\n\n4\n-40%\n\n\n5\n-50%\n\n\n6\n-60%\n\n\n7\n-70%\n\n\n8\n-80%\n\n\n9\n-90%\n\n\n10\nNo Credit",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "index.html#classroom-culture",
    "href": "index.html#classroom-culture",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "1.7 Classroom Culture",
    "text": "1.7 Classroom Culture\n\n1.7.1 Mission and Aims\nThe mission of Brigham Young University — founded, supported, and guided by The Church of Jesus Christ of Latter-day Saints — is to assist individuals in their quest for perfection and eternal life. That assistance should provide a period of intensive learning in a stimulating setting where a commitment to excellence is expected and the full realization of human potential is pursued.\nBYU seeks to develop students of faith, intellect, and character who have the skills and the desire to continue learning and to serve others throughout their lives.\nA BYU education should be (1) spiritually strengthening, (2) intellectually enlarging, and (3) character building, leading to (4) lifelong learning and service.\nBuilding on the foundational Mission and Aims, the Marriott School of Business aspires to transform the world through Christlike leadership by developing leaders of faith, intellect, and character guided by the following 4 values:\n\nFaith in Christ - We value deep and abiding faith in Jesus Christ. Our faith gives us the capacity to envision a better future, the confidence to make that future happen, and the courage to act in the face of challenges.\nIntegrity in Action - We value integrity and hold ourselves to the highest moral and ethical standards. Acting with integrity builds trust, strengthens character, and focuses our ambitions on things of eternal consequence.\nRespect for All - We value respect for all individuals as children of God and recognize the inherent worth, divine potential, and agency of each person. A climate of respect and belonging enhances our learning, facilitates collaboration, and encourages personal growth.\nExcellence - We value excellence in learning, teaching, research, management, and leadership. An expectation of excellence magnifies our influence and motivates us to continually improve.\n\nWe evaluate our decisions and actions by the impact they will have on the academic experience, professional preparation, character development, emotional well-being, and spiritual growth of our students.\n\n1.7.1.1 Prayer in class\nWe will begin each class with prayer. Each class member is invited to be voice for the prayer at least once throughout the semester. The TAs will reach out prior to class to invite you to pray on a particular day. If you’d rather not be voice for a prayer please let me know on the first day of class so I can instruct the TAs accordingly.\n\n\n1.7.1.2 Laptop Policy\nYou may use laptops in class for note taking or other class related purposes. Laptops should not be used for activities that would be a distraction to nearby students when your screen is in their line of sight (e.g. sports, instagram, etc.)\n\n\n1.7.1.3 Cold Calling\nI teach in a fairly conversational style, which includes cold calling students to ask for your input or to pose questions. If you’d rather I not cold call on you please let me know on the first day of class so that I can avoid doing so. I have deep respect for individual learning styles and will make accommodations when needed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "index.html#required-materials",
    "href": "index.html#required-materials",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "1.8 Required Materials",
    "text": "1.8 Required Materials\n\nBuild a Large Language Model From Scratch or here\n\nLMS: LearningSuite Class communication: Slack Required materials: The Lean Product Playbook: How to Innovate with Minimum Viable Products and Rapid Customer Feedback by Dan Olsen (physical copy recommended, $25 or less). Text and audio also available for free for BYU students digitally at O’Reilly books. Laptop or desktop (Mac or Windows) Install Mendix Studio Pro version 10.18.0 on Mac or Windows (free) Verify your student status with Figma and install desktop app for Mac or Windows (free)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "index.html#getting-help",
    "href": "index.html#getting-help",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "1.9 Getting Help",
    "text": "1.9 Getting Help\nThe following resources are available to get help: - Start with AI chat bots and the course website to see if they can assist - Use the course Slack channel to ask classmates for help - Attend TA or Professor office hours (coming soon) - Use Slack or email to contact one of the course TAs - Use Slack or email to contact Professor Murff",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "index.html#pre-enrollment-assessment",
    "href": "index.html#pre-enrollment-assessment",
    "title": "STRAT 490R - Understanding AI: From Foundations to Strategy",
    "section": "1.10 Pre-enrollment Assessment",
    "text": "1.10 Pre-enrollment Assessment\nComing soon.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "00-math-primer.html",
    "href": "00-math-primer.html",
    "title": "2  Math and Python Primer",
    "section": "",
    "text": "2.1 Calculus Essentials",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math and Python Primer</span>"
    ]
  },
  {
    "objectID": "00-math-primer.html#calculus-essentials",
    "href": "00-math-primer.html#calculus-essentials",
    "title": "2  Math and Python Primer",
    "section": "",
    "text": "2.1.1 Why You Need Calculus in ML/AI\nNeural networks are functions trained to minimize prediction errors. To train them, we need to compute how changes in weights affect the output — and for that, we use differentiation and the chain rule.\n\n\n2.1.2 Derivatives\nA measure of how a function changes as its input changes.\nNotation:\n\n\\(\\frac{dy}{dx}\\): derivative of output \\(y\\) with respect to input \\(x\\)\n\n\\(f'(x)\\): shorthand for “the derivative of function \\(f\\) at \\(x\\)”\n\nExample:\nIf \\(f(x) = x^2\\), then \\(f'(x) = 2x\\)\n\n\n2.1.3 Partial Derivatives\nA derivative with respect to one variable while keeping others constant.\nNotation:\n$ $\nUsed in computing how the loss changes with respect to each model parameter.\n\n\n2.1.4 Chain Rule\nUsed to compute derivatives of composed functions (e.g., layer-by-layer in a neural network).\nFormula:\n\\[\n\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n\\]\n\n\n2.1.5 Gradient\nA vector of all partial derivatives of a function with respect to each input.\nNotation:\n\\(\\nabla L = \\left[ \\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2}, \\ldots \\right]\\)\n\n\n2.1.6 Loss Function\nMeasures how wrong the model’s prediction is.\nExamples:\n- Mean Squared Error (MSE)\n- Cross-Entropy Loss\n\n\n2.1.7 Backpropagation\nAn algorithm that uses the chain rule to compute gradients efficiently in neural networks.\n\n\n2.1.8 Gradient Descent\nMethod used to update weights based on gradients.\nUpdate Rule:\n\\[\nw := w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n\\]\nWhere \\(\\eta\\) is the learning rate.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math and Python Primer</span>"
    ]
  },
  {
    "objectID": "00-math-primer.html#linear-algebra-essentials",
    "href": "00-math-primer.html#linear-algebra-essentials",
    "title": "2  Math and Python Primer",
    "section": "2.2 Linear Algebra Essentials",
    "text": "2.2 Linear Algebra Essentials\n\n2.2.1 Why You Need Linear Algebra in ML/AI\nNeural networks use vectors and matrices to represent data, weights, and transformations.\n\n\n2.2.2 Vectors\nA 1D array of numbers.\n$$ =\n\\[\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\]\n$$\n\n\n2.2.3 Matrices\nA 2D array of numbers.\n$$\nA =\n\\[\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}\\]\n$$\n\n\n2.2.4 Matrix-Vector Multiplication\nIf \\(A\\) is \\(m \\times n\\) and \\(\\vec{x}\\) is \\(n \\times 1\\), then \\(A \\vec{x}\\) is \\(m \\times 1\\).\n\n\n2.2.5 Dot Product\n\\(\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i b_i\\)\nUsed in similarity measures and basic model computations.\n\n\n2.2.6 Transpose\nSwitches rows and columns in a matrix.\n\\(A^T\\)\n\n\n2.2.7 Identity Matrix\nSquare matrix with 1s on the diagonal.\nActs like “1” for matrix multiplication.\n\\(AI = A\\)\n\n\n2.2.8 Matrix Multiplication\nCombines transformations.\nIf \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\), then \\(C = AB\\) is \\(m \\times p\\).\n\n\n2.2.9 Norms\nMeasure the size or length of a vector.\nL2 norm:\n$| |_2 = $",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math and Python Primer</span>"
    ]
  },
  {
    "objectID": "00-math-primer.html#probability-and-statistics-essentials",
    "href": "00-math-primer.html#probability-and-statistics-essentials",
    "title": "2  Math and Python Primer",
    "section": "2.3 Probability and Statistics Essentials",
    "text": "2.3 Probability and Statistics Essentials\n\n2.3.1 Why You Need Probability in ML/AI\nModels make predictions under uncertainty. Probability describes this uncertainty and informs how we evaluate and train models.\n\n\n2.3.2 Random Variables\nRepresent outcomes of random processes.\nDiscrete: number of heads\nContinuous: model confidence score\n\n\n2.3.3 Probability Distributions\nDiscrete: \\(P(X = x)\\)\nContinuous: \\(p(x)\\)\nExamples:\n- Bernoulli (binary outcomes)\n- Categorical (multi-class)\n- Gaussian/Normal (real-valued data)\n\n\n2.3.4 Expectation (Mean)\nDiscrete:\n\\(\\mathbb{E}[X] = \\sum_x x \\cdot P(X = x)\\)\nContinuous:\n\\(\\mathbb{E}[X] = \\int x \\cdot p(x) dx\\)\n\n\n2.3.5 Variance and Standard Deviation\n\\(\\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\\)\n\\(\\sigma = \\sqrt{\\mathrm{Var}(X)}\\)\n\n\n2.3.6 Conditional Probability\n$ P(A B) = $\nUsed in next-token prediction:\n\\(P(\\text{next token} \\mid \\text{context})\\)\n\n\n2.3.7 Bayes’ Theorem\n$ P(A B) = $\n\n\n2.3.8 Entropy\n$ H(X) = -_x P(x) P(x) $\nMeasures uncertainty in a distribution.\n\n\n2.3.9 Cross-Entropy Loss\n\\(\\text{Loss} = -\\sum_i y_i \\log(\\hat{y}_i)\\)\nUsed in classification and language modeling.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math and Python Primer</span>"
    ]
  },
  {
    "objectID": "00-math-primer.html#python-essentials-for-building-a-language-model",
    "href": "00-math-primer.html#python-essentials-for-building-a-language-model",
    "title": "2  Math and Python Primer",
    "section": "2.4 Python Essentials for Building a Language Model",
    "text": "2.4 Python Essentials for Building a Language Model\n\n2.4.1 Why Python?\nPython is widely used in AI due to its readability and powerful libraries like NumPy and PyTorch.\n\n\n2.4.2 Variables and Types",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math and Python Primer</span>"
    ]
  },
  {
    "objectID": "01-machine-learning-1.html",
    "href": "01-machine-learning-1.html",
    "title": "3  Machine Learning I",
    "section": "",
    "text": "3.1 Machine Learning in a Nutshell\nMachine learning is about enabling computers to recognize patterns in data so they can make accurate predictions or decisions without being explicitly programmed for every scenario. At its core, it involves feeding a model real-world examples encoded in a dataset and allowing the computer to learn the relationship between inputs and outputs. Once trained, the model can apply that learned relationship to make predictions on new, unseen data.\nTo build a machine learning model, we need four key ingredients:\nThere are many business use cases for machine learning, a few of which are listed below. We’ll start with the simplest possible machine learning model: simple linear regression. It’s a powerful tool that helps us understand the core ideas behind more complex models, including the ones that power today’s cutting-edge AI systems like ChatGPT and several of the examples in the table below.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Machine Learning I</span>"
    ]
  },
  {
    "objectID": "01-machine-learning-1.html#machine-learning-in-a-nutshell",
    "href": "01-machine-learning-1.html#machine-learning-in-a-nutshell",
    "title": "3  Machine Learning I",
    "section": "",
    "text": "A mathematical model: This defines the form of the mathematical function we’ll use to relate inputs to outputs, for example, a straight line (linear) or a more flexible structure like a neural network (non-linear).\n\n\n\n\n\n\n\nWhat do we mean by “linear”?\n\n\n\n\n\n“Linear” in this context means that the effect of \\(x\\) on \\(\\hat{y}\\) is proportional and constant: no matter what value of \\(x\\) we choose, an increase of 1 unit in \\(x\\) always increases \\(\\hat{y}\\) by exactly \\(w_1\\) units. This property makes the model highly interpretable.\n\n\n\n\nTraining data: A collection of real-world examples that pair inputs with their corresponding outputs. The quality and relevance of this data are crucial to how well the model can learn and make accurate predictions.\nA loss function: A mathematical expression that measures how far off the model’s predictions are from the correct answers. It provides feedback to help the model improve over time.\nA training algorithm: A step-by-step procedure that combines the first three ingredients in a way that minimizes the prediction errors produced by the model. This is where the so-called learning takes place.\n\n\n\n\n\n\n\n\n\n\n\nUse Case\nSample Inputs\nWhat does the model output?\nWhat Business Question is Being Answered?\n\n\n\n\nPrice Prediction\nProperty size, location, features\nNumeric price estimate\nHow should I price products to stay competitive and profitable?\n\n\nDemand Forecasting\nHistorical sales, promotions, holidays\nPredicted sales volume\nHow can I manage inventory or staffing to meet demand?\n\n\nCredit Risk Scoring\nIncome, credit history, employment data\nRisk category (e.g., low/medium/high)\nShould I approve this loan, and at what interest rate?\n\n\nEmail Spam Detection\nEmail content, sender info, subject line\nBinary label: spam or not spam\nHow can I prevent unwanted emails from reaching users’ inboxes?\n\n\nSentiment Analysis\nReview text, social media posts\nSentiment label (positive/negative)\nWhat is the public opinion about my product or brand?\n\n\nDisease Diagnosis\nSymptoms, test results, demographics\nDisease class (e.g., flu, COVID, none)\nHow can I assist doctors in making accurate and timely diagnoses?\n\n\nEnergy Usage Estimation\nTime of day, weather, appliance usage\nPredicted energy consumption\nHow should I manage power supply or optimize grid efficiency?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Machine Learning I</span>"
    ]
  },
  {
    "objectID": "01-machine-learning-1.html#simple-linear-regression",
    "href": "01-machine-learning-1.html#simple-linear-regression",
    "title": "3  Machine Learning I",
    "section": "3.2 Simple Linear Regression",
    "text": "3.2 Simple Linear Regression\n\n3.2.1 1st Ingredient: Mathematical model\nA simple linear regression model takes a single input variable \\(x\\) and predicts the value of a corresponding output variable \\(y\\). For example, the input variable might represent a house’s square footage, and the output variable could represent the value of the home.\nWe can write down the relationship between square footage and value in the form of a mathematical equation (also called a mathematical function or a model):\n\\[\ny = w_0 + w_1 x\n\\]\nWhere \\(y\\) represents the home value and \\(x\\) represents the square footage.\nIn machine learning jargon \\(w_0\\) and \\(w_1\\) are called the parameters or weights of the model (hence the use of \\(w\\) in the notation) and describe the nature of the relationship between \\(x\\) and \\(y\\). \\(w_0\\) and \\(w_1\\) are the numbers that the computer will learn (i.e. derive) based on what is observed in real life which will be encoded into training data discussed in the next section.\nIn other fields such as econometrics these might be introduced using the greek alphabet notation of \\(\\alpha\\) and \\(\\beta\\).\nYou might recognize this equation as the equation for a line in slope-intercept from which is often presented as:\n\\[\ny = mx + b\n\\]\nwhere \\(x\\) and \\(y\\) are numbers in the coordinate plane with \\(m\\) representing the slope of the line and \\(b\\) representing the y-intercept (the point where the line crosses the y-axis) as shown in the image below:\n\n\n\n\n\nSimple linear regression and the equation of a line are in fact the same mathematical equation. In the context of linear regression, we simply use different notation: \\(w_1 = m\\) (the slope), and \\(w_0 = b\\) (the y-intercept which in machine learning is referred to as the bias term).\nIt’s common for different fields of study to use different notation and words for the same mathematical concepts. Unfortunately this can be one of the biggest sources of confusion for students so we will make an effort to call out these differences throughout the course.\nNow that we’ve shown that simple linear regression is actually just the equation of a line where \\(w_0\\) is the y-intercept and \\(w_1\\) is the slope we can have a visual picture in mind for how an input variable \\(x\\) and the output variable \\(y\\) are related.\n\n\n\n\n\n\n\n\nSimple Linear Regression Line\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\n\n\n\n\nIntuition check\n\n\n\n\n\nThink about our home value example, where we’re predicting a house’s price based on its square footage.\n\nWhat does your intuition tell you about the values \\(w_0\\) and \\(w_1\\) are likely to take on once they are estimated?\nDo you expect them to be positive or negative numbers?\nAssume \\(x\\) was \\(0\\), what would the equation be telling you?\n\n\n\n\nAs soon as we come up with values for \\(w_0\\) and \\(w_1\\) have a way predicting values of \\(y\\) when plugging in any value of \\(x\\) to the fitted equation.\nThe goal of machine learning is to learn the best possible values \\(w_0\\) and \\(w_1\\) allowing us to make good predictions of a homes value based on it’s square footage. We will soon explore how the computer learns these weight values but to build our intuiton on what the model does we can start by simply guessing values for the weights. For example, let’s assume:\n\\[\nw_0 = 50,\\!000 \\quad \\text{and} \\quad w_1 = 200\n\\]\nThen our function becomes:\n\\[\n\\hat{y} = 50,\\!000 + 200x\n\\]\nWe use the notation \\(\\hat{y}\\) (read as “y-hat”) here to emphasize that this is a now predicted value based on the model, not an observed or actual value.\nThis act of using the trained model to compute a prediction based on an input \\(x\\) is also called inference since we are inferring an estimated output \\(\\hat{y}\\).\nUsing this model we can predict that a home with 3,000 square feet will have a value of:\n\\[\n\\hat{y} = 50,\\!000 + 200 \\cdot 3,\\!000 = \\$650,\\!000\n\\]\nNow, consider what the model predicts for a home with 0 square feet:\n\\[\n\\hat{y} = 50,\\!000 + 200 \\cdot 0 = \\$50,\\!000\n\\]\nThis implies that the base value of the property—the land alone, with no house—might be interpreted as $50,000. This is exactly why both \\(w_0\\) and \\(w_1\\) are necessary. If we had included only \\(w_1 x\\) and omitted \\(w_0\\), the model would always predict 0 for an input of \\(x = 0\\), which might not reflect the reality (e.g., land still has value).\nIn machine learning, the term bias is used to refer to this \\(w_0\\) value. The name comes from the fact that it shifts (or “biases”) the entire output of the model up or down, independent of the input. Geometrically, it determines the \\(y\\)-intercept of the prediction line. It allows the model to better fit real-world data.\nChoosing a different set of weight values would result in a different equation, resulting in a different prediction. For example, assume instead that \\(w_0 = 25,000\\) and \\(w_1 = 300\\) resulting in the following equation:\n\\[\n\\hat{y} = 25,000 + 300 x\n\\]\nThis model would predict that the same 3,000 square-foot home has a much higher value of \\(\\$925,000 = 25,000+300*3,000\\).\nLet’s assume the $3,000 square foot home we have in mind recently sold for $800,000 reflecting it’s true value (a single instance of \\(y\\)). We could then compute the error associated with each set of model weights as the absolute value of the prediction error as follows:\n\nPrediction error when \\(w_0 = 50,\\!000\\) and \\(w_1 = 200\\): \\(\\left| 650,000 - 8000000 \\right| = 150,000\\)\nPrediction error when \\(w_0 = 25,\\!000\\) and \\(w_1 = 300\\): \\(\\left| 925,000 - 8000000 \\right| = 125,000\\)\n\nGiven these results we might reasonably conclude that the second set of model weights produced the better prediction, since it was less wrong by $25,000.\nIn machine learning this prediction error is commonly called the model’s loss, that is, how far off the prediction is from the observed truth. The mathematical function by which we compute prediction error is called the loss function. In our case, we could represent our choice of loss function as \\(\\left| \\hat{y} - y \\right|\\).\nThere are many possible choices of loss function. For example, we didn’t have to use absolute value, we could have simply taken the difference as our measurement of loss. We will discuss loss functions and introduce the most commonly used loss functions in later sections.\n\n\n3.2.2 2nd Ingredient: Training Data\nIn order for a computer to learn the best weight values for a model, we need to communicate with it in the language it understands: data. In this context, “data” refers to numerical values organized in rows and columns, like a spreadsheet or matrix. Each row is a single example, and each column holds a particular feature or label.\nFor our home value prediction example, imagine a simple dataset with two columns:\n\nOne column for the square footage of a home (the input), and\n\nOne column for the value of the home (the output).\n\nEach row contains both the square footage and the corresponding value for a particular house. Together, each pair of values forms what we call an input-output pair. A complete set of these pairs is called a dataset.\nWe can write this dataset using the following compact mathematical notation:\n\\[\n\\{(x^{(i)}, y^{(i)})\\}_{i=1}^n\n\\]\nThis might look intimidating at first, but it’s just a convenient way of saying:\n\n“We have \\(n\\) examples. For each example \\(i\\), we observe an input \\(x^{(i)}\\) and a corresponding output \\(y^{(i)}\\).”\n\nLet’s break down the notation a bit further:\n\nThe curly braces \\(\\{ \\}\\) mean we’re describing a set, a collection of items.\nThe superscript \\((i)\\) just means “for example number \\(i\\)”\nEach item in the set is a pair: \\((x^{(i)}, y^{(i)})\\) where:\n\n\\(x^{(i)}\\) is the input (e.g., square footage) for the \\(i\\)th example\n\\(y^{(i)}\\) is the output (e.g., home value) for that same example\n\nThe subscript \\(_{i=1}^n\\) tells us there are \\(n\\) examples and we are referring to all \\(1\\) to \\(n\\) of them.\n\nIf you wrote this out as a table, it might look like this:\n\n\n\nExample (\\(i\\))\n\\(x^{(i)}\\) = Square Footage\n\\(y^{(i)}\\) = Home Value\n\n\n\n\n1\n1200\n250,000\n\n\n2\n1400\n275,000\n\n\n3\n1600\n300,000\n\n\n…\n…\n…\n\n\n\\(n\\)\n(last example)\n\n\n\n\nThis is the kind of data we use to “train” a machine learning model, by showing it many examples, we give it a chance to learn the relationship between inputs and outputs.\n\n\n\n\n\n\nTerminology: Inputs and Outputs\n\n\n\n\n\nIn different fields and contexts, we often use different terms for the same underlying ideas. Here’s a helpful reference for the various names used for inputs and outputs in machine learning and related areas:\n\n\n\n\n\n\n\n\nConcept\nCommon Synonyms\nNotes\n\n\n\n\nInput\nFeature, Independent Variable, Predictor, Covariate, Regressor, \\(x\\)\nThe value(s) we feed into the model to make a prediction. Can be one variable or many.\n\n\nOutput\nLabel, Target, Dependent Variable, Response, \\(y\\)\nThe value the model is trying to predict or learn from.\n\n\nInput-Output Pair\nExample, Observation, Data Point, \\((x^{(i)}, y^{(i)})\\)\nA single row of data showing both the input and the correct output.\n\n\nCollection of Examples\nDataset, Training Data, Sample\nAll the input-output pairs we give to the model to learn from.\n\n\nPredicted Output\nPrediction, Estimate, \\(\\hat{y}\\)\nThe output the model thinks is correct, based on what it learned.\n\n\n\n\n\n\nThere are actually four equivalent ways of representing a dataset, each useful in different contexts. You will be greatly aided in your study of machine learning if you can recognize and switch between all four forms with ease. Different textbooks, courses, tutorials and code libraries will use different representations so developing fluency in all of them will help you understand ideas more deeply and communicate more clearly.\n\nMathematical Set Notation:\n\nThis is the notation we just introduced:\n\\[\n\\{(x^{(i)}, y^{(i)})\\}_{i=1}^n\n\\]\nIt represents the dataset as a collection of \\(n\\) input-output pairs, where each \\(x^{(i)}\\) is an input and each \\(y^{(i)}\\) is the corresponding output. This form is widely used in textbooks and research papers because of its compactness and precision. It’s the language of mathematics, and it’s especially helpful when trying to understand what’s happening under the hood as computer code executes. By using this notation, we can reason more clearly about how models learn and make predictions.\n\nSpreadsheet or Table Format:\n\nThis is the most familiar form for most people and is often used in business and statistics. Each row represents an example; each column represents a variable or feature.\n\n\n\n\n\n\n\n\nExample (\\(i\\))\n\\(x^{(i)}\\) = Input (e.g., SqFt)\n\\(y^{(i)}\\) = Output (e.g., Price)\n\n\n\n\n1\n1200\n250,000\n\n\n2\n1400\n275,000\n\n\n3\n1600\n300,000\n\n\n…\n…\n…\n\n\n\\(n\\)\n—\n—\n\n\n\nThis is also the format used in tools like Excel, Google Sheets, or data frames in Python and R.\n\nMathematical Matrix Notation:\n\nMatrix notation is a compact and powerful way to represent datasets and linear regression models. It helps us generalize models to many inputs and use efficient numerical libraries for computation.\nIn matrix form, a simple linear regression with one input variable looks like this:\n\\[\n\\hat{y} =\n\\begin{bmatrix}\n1 & x\n\\end{bmatrix}\n\\begin{bmatrix}\nw_0 \\\\\nw_1\n\\end{bmatrix}\n= w_0 + w_1 x.\n\\]\nHere:\n\nThe “1” in the input vector allows us to represent the intercept term \\(w_0\\) together with the slope \\(w_1\\).\nThis convention turns the model into a dot product (vector multiplication).\n\nWhile this may initially feel like a notational trick, it is a standard approach in regression modeling and is essential for applying matrix operations efficiently.\nThis notation extends naturally to represent the entire training dataset. When including an intercept, we create an input matrix (also called a design matrix) by adding a column of ones on the left:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & x^{(1)} \\\\\n1 & x^{(2)} \\\\\n\\vdots & \\vdots \\\\\n1 & x^{(n)}\n\\end{bmatrix},\n\\] More generally:\n\nThe design matrix \\(\\mathbf{X}\\) has shape \\((n, d + 1)\\), where:\n\n\\(n\\) = number of training examples (rows).\n\\(d\\) = number of input features.\nThe extra column of ones accounts for the intercept (also called the bias term).\n\n\nIf we have more than one input feature (for example, square footage and number of bedrooms, etc), \\(\\mathbf{X}\\) becomes:\n\\[\n\\begin{bmatrix}\n1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_d^{(1)} \\\\\n1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_d^{(2)} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_1^{(n)} & x_2^{(n)} & \\cdots & x_d^{(n)}\n\\end{bmatrix}\n\\]\nThe output vector \\(\\hat{\\mathbf{y}}\\) stores the predicted values (e.g., house prices):\n\\[\ny =\n\\begin{bmatrix}\ny^{(1)} \\\\\ny^{(2)} \\\\\n\\vdots \\\\\ny^{(n)}\n\\end{bmatrix}\n\\]\nThe weight vector \\(\\mathbf{w}\\) stores the weight parameters:\n\\[\n\\mathbf{w} =\n\\begin{bmatrix}\nw_0 \\\\\nw_1 \\\\\n\\vdots \\\\\nw_{d}\n\\end{bmatrix}\n\\] This leads to the compact matrix equation for all predictions:\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{w}\n=\n\\begin{bmatrix}\n1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_d^{(1)} \\\\\n1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_d^{(2)} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_1^{(n)} & x_2^{(n)} & \\cdots & x_d^{(n)}\n\\end{bmatrix}\n\\begin{bmatrix}\nw_0 \\\\\nw_1 \\\\\nw_2 \\\\\n\\vdots \\\\\nw_d\n\\end{bmatrix}\n\\] &gt; Matrix notation helps us express models clearly and allows computers to compute predictions for all examples at once. This approach is essential when scaling to many features and large datasets. We will rely on this notation throughout the course.\n\nCode Representation (Arrays or Tensors):\nWhen implementing models in code, we typically use arrays (Python’s NumPy package) or tensors (Python’s PyTorch package) which are data structures that store values in memory for numerical computation.\nBelow is an example with NumPy\n\n\n# NumPy\nimport numpy as np\nX = np.array([[1200], [1400], [1600]])\ny = np.array([[250000], [275000], [300000]])\nprint(\"X =\", X)\nprint(\"y = \", y)\n\nX = [[1200]\n [1400]\n [1600]]\ny =  [[250000]\n [275000]\n [300000]]\n\n\nSo what does this mean?\n\nX is a matrix of inputs (in this case, just one feature: square footage).\nEach row represents one example:\n\n1200 square feet\n\n1400 square feet\n\n1600 square feet\n\ny is a vector of outputs (the target values, like home prices).\nEach row in y matches the corresponding row in X.\n\nLet’s now connect this code back to the set and matrix representations of a dataset.\nEquivalence with Set Notation\nIn set notation this tiny dataset could be represented as three input-output pairs:\n\\[\n\\{(x^{(i)}, y^{(i)})\\}_{i=1}^3\n\\] More specifically:\n\n\\(x^{(1)} = 1200\\), \\(y^{(1)} = 250{,}000\\)\n\n\\(x^{(2)} = 1400\\), \\(y^{(2)} = 275{,}000\\)\n\n\\(x^{(3)} = 1600\\), \\(y^{(3)} = 300{,}000\\)\n\nEach pair \\((x^{(i)}, y^{(i)})\\) is represented by a row in X and the corresponding row in y.\nEquivalence with Matrix Notation\nWe can also see how this toy dataset would be written in matrix notation as a pair of matrices, one for the inputs and one for the outputs:\n\\[\nX =\n\\begin{bmatrix}\nx_1^{(1)} \\\\\nx_1^{(2)} \\\\\nx_1^{(3)}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1200 \\\\\n1400 \\\\\n1600\n\\end{bmatrix},\n\\quad\ny =\n\\begin{bmatrix}\ny^{(1)} \\\\\ny^{(2)} \\\\\ny^{(3)}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n250{,}000 \\\\\n275{,}000 \\\\\n300{,}000\n\\end{bmatrix}\n\\]\n\n\\(X\\) is an \\(n \\times 1\\) input matrix (with one feature per example),\n\\(y\\) is an \\(n \\times 1\\) output vector (with one label per example).\n\nPyTorch Example\nLastly, below we represent the same toy dataset using PyTorch, currently the most popular deep learning library, and the one used to develop many modern large language models (LLMs).\nPyTorch uses a data structure called a tensor.\nA tensor is a generalization of familiar objects like scalars, vectors, and matrices:\n\nA scalar is a 0-dimensional tensor (e.g., 5)\nA vector is a 1-dimensional tensor (e.g., [1200, 1400, 1600])\nA matrix is a 2-dimensional tensor (e.g., a table of numbers)\nA 3D tensor is a collection or stack of matrices. You can think of it as a book of matrices, where each page is a 2D grid (matrix), and the whole book has depth (i.e., the number of pages)\nA 4D tensor is a collection of 3D tensors. You can think of it as a library of books, where:\n\nEach book is a 3D tensor (a stack of matrices),\nEach page in a book is a matrix,\nAnd each matrix contains rows and columns of numbers.\n\n\nIncreasing the tensor dimension allows us to compactly describe multiple sets of structured data and for a computer to perform parallel computations efficiently which is essential when training modern LLMs.\nIn theory, there is no limit on the number of dimensions a tensor can have.\nIn practice, we won’t need more than 4D tensors to build modern large language models (LLMs).\nFor simple models like linear regression, a 2D tensor (i.e., a matrix) is sufficient to represent the data.\nBelow we illustrate the same toy data set using a PyTorch tensor.\n\nimport torch\n\n# Input data: square footage (in one column)\nX = torch.tensor([[1200.], [1400.], [1600.]])\n\n# Output data: home prices\ny = torch.tensor([[250000.], [275000.], [300000.]])\n\nprint(\"X (inputs):\")\nprint(X)\n\nprint(\"\\ny (outputs):\")\nprint(y)\n\nX (inputs):\ntensor([[1200.],\n        [1400.],\n        [1600.]])\n\ny (outputs):\ntensor([[250000.],\n        [275000.],\n        [300000.]])\n\n\nThe dot (.) at the end of the numbers (like 1200. or 250000.) indicates that the numbers are being treated as floating point numbers (i.e., float type) rather than integers. A floating point number is a number that can represent decimal values on a computer, in contrast to an integer, which can only represent whole numbers. PyTorch models expect inputs and outputs to be floating point numbers because most model operations involve decimals.\nWe will soon use a set of training Data to help learn model parameters but before doing so we need to introduce the remaining two ingredients for machine learning: loss functions and training algorithms.\n\n\n3.2.3 3rd Ingredient: Loss Function\nA loss function is separate mathematical equation whose purpose it to quantify the error between a model’s predictions \\(\\hat{y}^{(i)}\\) and the true output values observed in the training data \\(y^{(i)}\\). When discussing mathematical model structure (i.e. simple linear regression), we briefly introduced the notion of a loss function and used the absolute value of the difference between \\(\\hat{y}^{(i)}\\) and between and \\(y^{(i)}\\) to measure prediction error (i.e. loss): \\(\\left| \\hat{y} - y \\right|\\). Note that this loss is calculated for each individual training example. That is, if we had 100 training examples (i.e. rows of data) containing the square footage and home value for 100 homes, we could compute the loss for each of these examples. The dotted lines in the plot below help you visualize the loss for each example in a set of training data.\n\n\n\n\n\nWe could then find the average loss by summing up the observation level loss and dividing by the number of observations to get what is commonly called Mean Absolute Error (MAE), written as:\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\left| \\hat{y}^{(i)} - y^{(i)} \\right|\n\\]\nMAE is an intuitive choice for a loss function, but not the only choice. There are other equations that will measure prediction error in slightly different ways. One such equation and the most commonly used loss function when learning parameters for a simple linear regression is called Mean Squared Error (MSE), written as:\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\n\\]\nRecall that \\(\\hat{y} = w_0 + w_1 x\\) hence we can plug \\(w_0 + w_1 x\\) into the equation for \\(\\hat{y}\\) to make clear that the loss function, which we denote as \\(\\mathcal{L}\\) (a stylized version of the capital Latin letter L), is a function of two variables: \\(w_0\\) and \\(w_1\\). This means that different values of \\(w_0\\) or \\(w_1\\) or both will yield different loss values. The goal of machine learning is to find (i.e. learn) the values of \\(w_0\\) and \\(w_1\\) that will make this loss function (i.e., total prediction error) as small as possible.\n\\[\n\\mathcal{L}(w_0, w_1) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{n} \\sum_{i=1}^n (w_0 + w_1 x^{(i)} - y^{(i)})^2\n\\]\nIn the next section, we will introduce a training algorithm called gradient descent. This algorithm systematically searches through different combinations of values for \\(w_0\\) and \\(w_1\\) to find those that minimize the loss or in other words, to learn the best-fitting line.\nTo really understand how gradient descent works, we first need to deepen our understanding of the loss function, specifically how it behaves with respect to the parameters \\(w_0\\) and \\(w_1\\).\nRecall that in simple linear regression, we model predictions as:\n\\[\n\\hat{y} = w_0 + w_1 x\n\\]\nWhen we think of this as a function of the input variable \\(x\\), we can plot it as a straight line on a 2D coordinate system (with \\(x\\) on the horizontal axis and \\(\\hat{y}\\) on the vertical axis) as we did earlier.\nBut now, let’s flip our perspective. In the previous section, we learned that training data consists of actual examples, pairs of \\(x\\) and \\(y\\), from the real world. So once we have a training dataset, \\(x\\) and \\(y\\) are known. That means we no longer need to treat \\(x\\) as a variable in our model equation.\nIn this case, the prediction formula:\n\\[\n\\hat{y} = w_0 + w_1 x\n\\]\nbecomes a function where \\(w_0\\) and \\(w_1\\) are the unknown variables. Our goal is to adjust these weights to reduce the difference between our predicted values \\(\\hat{y}\\) and the actual outcomes \\(y\\). So during model training, we treat \\(w_0\\) and \\(w_1\\) as the variables to solve for, which turns our loss function into a function of two variables, \\(w_0\\) and \\(w_1\\):\n\\[\n\\mathcal{L}(w_0, w_1) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{n} \\sum_{i=1}^n (w_0 + w_1 x^{(i)} - y^{(i)})^2\n\\] Don’t be intimidated by the notation. Once we have a training dataset, all the \\(x^{(i)}\\) and \\(y^{(i)}\\) values are known values. The only unknowns we are solving for are \\(w_0\\) and \\(w_1\\).\n\n\n\n\n\n\nUnpacking the notation of the MSE Loss Function\n\n\n\n\n\nThis is called the Mean Squared Error (MSE) loss function, and it helps us measure how good or bad our model’s predictions are.\nThe big curly \\(\\mathcal{L}\\) is a stylized version of the capital Latin letter L. In this context, \\(\\mathcal{L}\\) stands for “Loss” or sometimes “Loss function.”\n\n\\(w_0\\) and \\(w_1\\) are the parameters (or weights) of our simple linear regression model. Notice that they are playing the roles of unknown variables in this notation: \\(\\mathcal{L}(w_0, w_1)\\). This is because we don’t know what these values are or should be yet, that’s what we want the computer learn for us.\n\\(x^{(i)}\\) is the input (e.g., square footage of a house)\n\\(\\hat{y}^{(i)} = w_0 + w_1 x^{(i)}\\) is the model’s predicted output (e.g., the predicted house price).\n\\(y^{(i)}\\) is the true value (the actual house price for data point \\(i\\)).\n\\((\\hat{y}^{(i)} - y^{(i)})^2\\) is the squared error: how far off the prediction is, squared to make sure it’s always positive and to penalize big mistakes more than small ones.\n\\(\\sum_{i=1}^n\\) adds up the squared errors for all \\(n\\) data points in our dataset (the Greek letter \\(\\sum\\) is called “capital sigma” and means we should “add up a bunch of terms”)\n\\(\\frac{1}{n}\\) takes the average of all those squared error terms.\n\nThis function tells us how well our model is performing overall. A smaller value means our predictions on average are close to the true values, and a larger value means on average we’re making bigger mistakes.\nOur goal is to find the values of \\(w_0\\) and \\(w_1\\) that make this loss function as small as possible.\n\n\n\nBecause the loss depends on both \\(w_0\\) and \\(w_1\\), we need three dimensions to visualize and plot this function:\n\nOne axis for \\(w_0\\)\n\nOne axis for \\(w_1\\)\n\nA vertical axis for the loss value \\(\\mathcal{L}\\) (often labeled the \\(z\\)-axis in other contexts)\n\nAs the saying goes, a picture is worth a thousand words, so let’s visualize what this looks like.\nIn the plot below, we graph the loss function \\(\\mathcal{L}(w_0, w_1)\\) to see it as a 3D surface, with \\(w_0\\) on the x-axis, \\(w_1\\) on the y-axis, and the height of the surface representing the mean squared error loss \\(\\mathcal{L}\\). This gives us a landscape of possible values of \\(w_0\\) and \\(w_1\\) to choose from.\nThe plot also includes contour lines, which are like the topographical lines you see on a map. These contours help you visualize the height of the surface even when looking down from above onto the flat 2D plane. The contour lines will be especially helpful in the next section when we explore how adjusting the weights changes the loss value.\n\n\n        \n        \n        \nMSE Loss Surface with Contours\n\n\n                            \n                                            \n\n\nAs can be seen from the plot, there are many possible choices of \\(w_0\\) and \\(w_1\\), each resulting in a different loss value. In the next section, we will study a foundational algorithm called gradient descent which is designed to intelligently and iteratively explore different combinations of \\(w_0\\) and \\(w_1\\), calculating the loss that results from each combination. The process continues until convergence occurs, that is, until the algorithm believes it has found the best choice of weights that minimize the loss. We’ll explore the details of how gradient descent works in the next section.\n\n\n\n\n\n\nIntuition Check\n\n\n\n\n\nWhich choice of weight values is better: point A or point B? Why?\n\n\n\n\n\n\n\n\n\nWhy use MSE for the loss function?\n\n\n\n\n\nMSE is the most commonly used loss function for regression problems because it has several useful properties:\n\nSmooth and differentiable: The squaring operation makes the MSE function smooth and continuous, which is important for optimization methods like gradient descent that rely on computing derivatives. (discussed in the next section)\nPenalizes large errors: By squaring the difference between prediction and truth, MSE emphasizes larger errors more than smaller ones. This is often desirable when big mistakes are especially costly.\nUnique minimum: For linear models, the MSE loss function is convex (it’s bowl shaped), meaning it has a single global minimum. This makes optimization straightforward and ensures reliable convergence during training.\n\nThese properties make MSE both practical and theoretically sound for training models to make accurate predictions.\n\n\n\n\n\n3.2.4 4th Ingredient: Training Algorithm\nOnce we’ve selected a loss function, the next challenge is figuring out how to minimize it, that is, how to find the values of \\(w_0\\) and \\(w_1\\) that make the loss as small as possible.\nTo build intuition, imagine you’re standing somewhere on the inside of a smooth, canyon-shaped surface like the 3D plot shown in the last section. This surface represents the loss function, and your location corresponds to a specific pair of weights: \\((w_0, w_1)\\). Your goal is to hike down to the very bottom of the canyon, the point where the loss is lowest and your model makes its best predictions.\nBut how do you know which way to step?\nThis is where the training algorithm called gradient descent comes in. It uses the concept of a derivative (a rate of change from calculus) to calculate the slope of the loss surface at your current position. Then it suggests a small step in the direction that will reduce the loss the fastest: the steepest descent.\nIt’s called an algorithm because it repeats this process: take a step, recalculate the slope, take another step while gradually working its way toward the minimum.\nIn this way, gradient descent acts like a smart compass that always points you downhill in the direction that leads most quickly to better predictions.\nThat explains what descent means in gradient descent, but what about gradient?\nGradient is just a fancy word for the direction of steepest ascent (what we’d call the slope in single variable calculus). Since we want to go downhill, toward lower loss, we will actually move in the opposite (i.e. negative) direction of the gradient, hence the name gradient descent.\nSince we are working with a loss function of two variables (the weights), \\(\\mathcal{L}(w_0, w_1)\\), there are two directions in play for each step we take down the hill. In this case, the gradient is a vector with two components, each representing the rate of change of the loss with respect to one of the weights (you can think of each component as the “slope” in a particular direction). Mathematically, these components are written as:\n\\(\\frac{\\partial \\mathcal{L}}{\\partial w_0}\\): how the loss changes when we adjust \\(w_0\\) \\(\\frac{\\partial \\mathcal{L}}{\\partial w_1}\\): how the loss changes when we adjust \\(w_1\\)\nTogether, these two components form the gradient vector of the loss function.\nThis is written using the nabla symbol \\(\\nabla\\) (pronounced “nabla”, named after an ancient harp), which represents the gradient operator:\n\\[\n\\nabla \\mathcal{L}(w_0, w_1) =\n\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial w_0} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_1}\n\\end{bmatrix}\n\\]\nThe notation looks intimidating, but let’s break it down symbol by symbol:\n\n\\(\\nabla\\) is a mathematical operator implying an action is being taken, in this case, computing a series of partial derivatives with respect to each weight\nThe partial derivative symbol \\(\\partial\\) (pronounced “partial”) means we’re measuring the rate of change of the loss \\(\\mathcal{L}\\) with respect to just one variable \\(w_0\\) or \\(w_0\\), keeping the other constant.\n\\(\\mathcal{L}\\) (script L) represents the loss function, which tells us how far off our model’s predictions are from the actual outcomes.\n\\(w_0\\) and \\(w_1\\) are the parameters of our model.\n\\(\\begin{bmatrix}\\end{bmatrix}\\) combines all components into a single vector\n\nJust like other more familiar mathematical operators, such as addition \\(+\\) and multiplication \\(\\times\\), the \\(\\nabla\\) operator means we will perform some operation on whatever equation or numbers we are applying to. In our case, we are taking the partial derivatives of the loss function with respect to each weight in order to obtain the gradient vector. Below, we will show how the partial derivitives of the loss function using rules from calculus.\n\n\n\n\n\n\nWhat is a derivative?\n\n\n\n\n\nA derivative is a way of measuring how fast something is changing.\nIn simple terms, the derivative tells you the slope of a function at a specific point—that is, how steep the function is at that spot.\nImagine this:\nYou’re walking up a hill, and you want to know how steep it is right where you’re standing. That steepness, the instantaneous slope, is the derivative.\n\nIf the slope is positive, you’re going uphill.\nIf the slope is negative, you’re going downhill.\nIf the slope is zero, you’re on flat ground (maybe the top or bottom of the hill).\n\nMathematically:\nIf \\(f(x) = x^2\\), then the derivative is:\n\\[\n\\frac{df}{dx} = 2x\n\\]\nAt \\(x = 3\\), the slope is \\(2 \\cdot 3 = 6\\), so the curve is rising steeply.\nIn gradient descent, we’re just following the steepest slope as we hike downhill toward the lowest point.\n\n\n\n\n\n\n\n\n\nWhat is a partial derivative?\n\n\n\n\n\nA partial derivative is just the derivative of a multivariable function with respect to one variable at a time, while holding all the other variables constant.\nThink of it like asking:\n&gt; “If I only adjust this one knob (say, \\(w_0\\)), how does the output change, assuming all other knobs stay fixed?”\nExample:\nSuppose we have this function with two variables:\n\\[\nf(w_0, w_1) = w_0^2 + 3w_0 w_1 + w_1^2\n\\]\nWe can take the partial derivatives:\n\nWith respect to \\(w_0\\): \\[\n\\frac{\\partial f}{\\partial w_0} = 2w_0 + 3w_1\n\\]\nWith respect to \\(w_1\\):\n\\[\n\\frac{\\partial f}{\\partial w_1} = 3w_0 + 2w_1\n\\]\n\nEach tells us how the function changes in that direction, treating the other variable like a constant.\nThis is essential for gradient descent: we compute the slope in each direction (each weight), then use those partials to form the gradient vector.\n\n\n\nRecall that our mean squared error (MSE) loss function is:\n\\[\n\\mathcal{L}(w_0, w_1) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{n} \\sum_{i=1}^n (w_0 + w_1 x^{(i)} - y^{(i)})^2\n\\] To take the partial derivatives we apply the chain rule which gives us:\n\n\n\n\n\n\nWhat is the Chain Rule\n\n\n\n\n\nThe chain rule is a way to take the derivative of a function inside another function.\nThink of it like peeling an onion: you take the derivative of the outer layer, then multiply it by the derivative of the inner layer.\nExample:\nSuppose you have:\n\\[\nf(x) = (3x + 1)^2\n\\]\nThis is a composition of two functions:\n\nOuter function: \\(u^2\\)\nInner function: \\(u = 3x + 1\\)\n\nUsing the chain rule:\n\\[\n\\frac{df}{dx} = \\frac{d}{du}(u^2) \\cdot \\frac{du}{dx} = 2u \\cdot 3 = 2(3x + 1) \\cdot 3 = 6(3x + 1)\n\\]\nSo even though the original function looks complicated, we can compute the derivative one layer at a time and then multiply the results from each layer.\nThis same principle is applied to find the partial derivatives of a loss function like MSE.\n\n\n\nPartial Derivative with respect to \\(w_0\\)\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial w_0} = \\frac{2}{n} \\sum_{i=1}^n (w_0 + w_1 x^{(i)} - y^{(i)}) \\times 1 = \\frac{2}{n} \\sum_{i=1}^n (w_0 + w_1 x^{(i)} - y^{(i)})\n\\]\nPartial Derivative with respect to \\(w_1\\)\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{2}{n} \\sum_{i=1}^n (w_0 + w_1 x^{(i)} - y^{(i)}) \\times x^{(i)}\n\\]\nPutting the two partial derivtives into a vector gives us the gradient of the loss function:\n\\[\n\\nabla \\mathcal{L}(w_0, w_1) =\n\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial w_0} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_1}\n\\end{bmatrix}\n=\n\\frac{2}{n} \\sum_{i=1}^n\n\\begin{bmatrix}\nw_0 + w_1 x^{(i)} - y^{(i)} \\\\\n(w_0 + w_1 x^{(i)} - y^{(i)}) \\cdot x^{(i)}\n\\end{bmatrix}\n\\] Don’t lose sight of the fact that this apparently complex notation just represents simple numbers in the end.\nFor example, let’s assume our training data was:\n\n\\(x^{(1)} = 0, \\; y^{(1)} = 1\\)\n\n\\(x^{(2)} = 1, \\; y^{(2)} = 3\\)\n\n\\(x^{(3)} = 2, \\; y^{(3)} = 5\\)\n\n\\(x^{(4)} = 3, \\; y^{(4)} = 7\\)\n\nWith initial weights of: \\(w_0 = 1, \\; w_1 = 1\\)\nThen, substituting all these values into the gradient equation above we get:\n\\[\n\\begin{align*}\n&= \\frac{2}{4} \\Bigg(\n\\begin{bmatrix}\n1 + 1 \\cdot 0 - 1 \\\\\n(1 + 1 \\cdot 0 - 1) \\cdot 0\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1 + 1 \\cdot 1 - 3 \\\\\n(1 + 1 \\cdot 1 - 3) \\cdot 1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1 + 1 \\cdot 2 - 5 \\\\\n(1 + 1 \\cdot 2 - 5) \\cdot 2\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1 + 1 \\cdot 3 - 7 \\\\\n(1 + 1 \\cdot 3 - 7) \\cdot 3\n\\end{bmatrix}\n\\Bigg) \\\\\n&= \\frac{2}{4} \\left(\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n-1 \\\\\n-1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n-2 \\\\\n-4\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n-3 \\\\\n-9\n\\end{bmatrix}\n\\right) \\\\\n&= \\frac{2}{4}\n\\begin{bmatrix}\n-6 \\\\\n-14\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n-3 \\\\\n-7\n\\end{bmatrix}\n\\end{align*}\n\\]\nSo the negative gradient of the loss function calculated at:\n\\[\n\\begin{bmatrix}w_0 \\\\ w_1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\\]\nis:\n\\[\n\\nabla \\mathcal{L}(w_0, w_1) =\n\\begin{bmatrix}\n3 \\\\\n7\n\\end{bmatrix}\n\\]\nThe weight vector and the gradient vector always have the same number of components because the gradient is composed of the partial derivatives with respect to each weight, one for every weight parameter.\nOur goal is to use the information enocoded in the gradient vector to update the weight vector and reduce the loss.\nWe will first explore what this looks like geometrically in order to build our intuition and then we will introduce the mathematical equation that performs this task for us which is the core of the gradient descent algorithm.\nThe left plot below shows the initial weight vector originating from the origin (point \\((0,0)\\)), and then shows the gradient descent vector (the negative of the gradient) orginating from the tip of the weight vector since this is the point at which the gradient is calculated.\nThe left plot also shows the combination of weight values that would result in minimum loss. This is ultimately where we’d like to get to. The dashed line shows the resulting new weight vector when we sum of the existing weight vector with the gradient descent vector. Notice how the gradient descent vector shoots straight through the minimum (i.e the direction of steepest descent), but unfortunately it leaps over it, missing our goal and actually taking us to a place of higher loss. The right plot shows how we address this by introducing a learning rate which is described next.\n\n\n                            \n                                            \n\n\nBecause of this risk of overshooting the minimum by taking two large of a step, we introduce a concept called a learning rate that governs the size of the step that we take. The learning rate is a number generally between 0 and 1 that shrinks the magnitude (length) of the gradient descent vector while maintaining its direction. Mathematically, the step size can be written as the product of the learning rate (a scalar) and the gradient vector:\n\\[\n\\text{Step size} = \\eta \\cdot \\nabla L(\\mathbf{w})\n\\]\nWhere:\n\n\\(\\eta\\) (the Greek letter “eta”) is the learning rate, a positive scalar that determines how big a step we take\n\\(\\nabla L(\\mathbf{w})\\) is the gradient of the loss function evaluated at the current weight vector \\(\\mathbf{w}\\).\n\nto make our notation more compact and prepare for situations where there are more than two weights, we introduce bold face vector notation where a bold \\(\\mathbf{w}\\) represents a vector of all the weights:\n\n\n\\[\n\\mathbf{w}\n=\n\\begin{bmatrix}\nw_0 \\\\\nw_1\n\\end{bmatrix}\n\\] To update the weights, we add the the weight vector to the negative scaled gradient (calculated at the current weights). This is equivalent to subtraction which is generally how you will see the update rule written:\n\\[\n\\mathbf{w}_{\\text{old}} + (- \\eta \\cdot \\nabla L(\\mathbf{w}_{\\text{old}})) = \\mathbf{w}_{\\text{old}} - \\eta \\cdot \\nabla L(\\mathbf{w}_{\\text{old}})\n\\]\n\\[\n\\mathbf{w}_{\\text{new}} \\leftarrow \\mathbf{w}_{\\text{old}} - \\eta \\cdot \\nabla L(\\mathbf{w}_{\\text{old}})\n\\]\nThis equation is the heart of gradient descent. It tells us:\n\nCalculate how the loss changes in response to each weight (the gradient).\nScale this change by the learning rate to control the step size.\nMove the weights in the opposite direction of the gradient to reduce the loss (geometrically adding the current weight vector with the negative of the gradient)\n\nOver multiple iterations, this process gradually guides the weights toward the combination that minimizes the loss function, ideally reaching or approaching the global minimum.\nIn practice, choosing an appropriate learning rate \\(\\eta\\) is critical:\n\nIf \\(\\eta\\) is too large, the steps may overshoot the minimum and cause divergence or oscillation.\nIf \\(\\eta\\) is too small, convergence will be very slow, requiring many updates to reach a good solution.\n\nWe can now summarize the gradient desecent algorithm:\n\n\n\n\n\n\nSummary of the Gradient Descent Algorithm\n\n\n\n\nInitialize weights\nStart with random or zero values for the components of the weight vector \\(\\mathbf{w}\\).\nCompute predictions\nUse the current weights to compute predictions \\(\\hat{y}\\) for all training examples.\nEvaluate the loss\nMeasure how far off the predictions are from the true values using a loss function \\(\\mathcal{L}(\\mathbf{w})\\).\nCompute the gradient\nCalculate the gradient \\(\\nabla \\mathcal{L}(\\mathbf{w})\\), which points in the direction of steepest increase in loss.\nUpdate the weights\nMove the weights in the opposite direction of the gradient using the update rule:\n\\[\n\\mathbf{w}_{\\text{new}} \\leftarrow \\mathbf{w}_{\\text{old}} - \\eta \\cdot \\nabla L(\\mathbf{w}_{\\text{old}})\n\\]\nRepeat\nRepeat steps 2–5 until the loss stops decreasing (or decreases very slowly), indicating convergence.\n\n\n\n\nThe gradient vector lives in the same 2D space as the weight vector, because it is made up of the partial derivatives with respect to each weight. If the weight vector has two components, the gradient vector will also have two components — one for each direction in weight space. Visually, the gradient can be drawn as an arrow on the 2D floor, pointing in the direction of steepest ascent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Machine Learning I</span>"
    ]
  },
  {
    "objectID": "01-machine-learning-1.html#glossary",
    "href": "01-machine-learning-1.html#glossary",
    "title": "3  Machine Learning I",
    "section": "3.3 Glossary",
    "text": "3.3 Glossary\nSimple Linear Regression\ntraining data\nmachine learning algorithm\nparameters or “weights”\nparameters or “weights”\nmachine learning algorithm\ninference\nsupervised machine learning\nEpoch\nSupervised Learning: Supervised learning is the most widely used type of machine learning and significantly overlaps with the methodologies of other quantitative fields, such as statistics and econometrics. Supervised learning gets its name from the fact that the input data set has examples of both inputs and outputs. The outputs are called labels, hence you will sometime hear the term “labeled dataset.”\n\n3.3.1 Synonyms Across Fields\nDifferent disciplines often use different terms for the same concepts. Here’s a helpful guide:\n\n\n\n\n\n\n\n\nConcept\nMachine Learning\nStatistics / Econometrics\n\n\n\n\nSingle data point (e.g., a row in a spreadsheet)\nTraining example\nObservation, Case\n\n\nInput features\nFeatures, Inputs\nIndependent variables, Regressors, Covariates, Predictors\n\n\nTarget output\nLabel, Target, Output\nDependent variable, Response\n\n\nCollection of data points\nTraining set, Training data\nSample, Dataset\n\n\nParameter\nWeight (e.g., \\(w_1\\))\nCoefficient (e.g., \\(\\beta_1\\))\n\n\nModel prediction\n\\(\\hat{y}\\)\nFitted value, Predicted value\n\n\nLoss function\nLoss\nObjective function, Criterion function\n\n\nLearn model parameters\nTraining, Fitting\nEstimation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Machine Learning I</span>"
    ]
  },
  {
    "objectID": "01-machine-learning-1.html#appendix",
    "href": "01-machine-learning-1.html#appendix",
    "title": "3  Machine Learning I",
    "section": "3.4 Appendix",
    "text": "3.4 Appendix\n\n3.4.1 Closed-form Solution (Normal Equation)\nWe can directly compute the optimal weights:\n\\[\n\\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Machine Learning I</span>"
    ]
  },
  {
    "objectID": "02-machine-learning-2.html",
    "href": "02-machine-learning-2.html",
    "title": "4  Machine Learning II",
    "section": "",
    "text": "4.1 Multiple Linear Regression",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning II</span>"
    ]
  },
  {
    "objectID": "02-machine-learning-2.html#logistic-regression",
    "href": "02-machine-learning-2.html#logistic-regression",
    "title": "4  Machine Learning II",
    "section": "4.2 Logistic Regression",
    "text": "4.2 Logistic Regression\n\n4.2.1 Classification",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning II</span>"
    ]
  },
  {
    "objectID": "02-machine-learning-2.html#overfitting-and-underfitting",
    "href": "02-machine-learning-2.html#overfitting-and-underfitting",
    "title": "4  Machine Learning II",
    "section": "4.3 Overfitting and Underfitting",
    "text": "4.3 Overfitting and Underfitting\n\nUnderfitting: model too simple to capture data patterns\nOverfitting: model too complex, captures noise instead of signal\nValidation helps detect these behaviors.\n\n\n4.3.1 Unsupervised Learning Use Cases\n\n\n\n\n\n\n\n\n\n\n\nUse Case\nSample Inputs\nModel Output Description\nWhat ML Question is Being Answered?\nWhat Business Question is Being Answered?\nExample Algorithm(s)\n\n\n\n\nCustomer Segmentation\nAge, income, purchase history\nCluster/group labels for each customer\nWhat types of customers exist in my data?\nHow can I tailor marketing strategies to different customer types?\nK-means, DBSCAN\n\n\nTopic Modeling\nArticles or documents\nTopics with keywords per document\nWhat topics are being discussed?\nWhat content themes resonate most with my audience or market?\nLDA, NMF\n\n\nAnomaly Detection\nTransaction logs, sensor data\nAnomaly score or binary flag\nWhich data points are unusual?\nAre there fraudulent transactions or system failures I need to act on?\nIsolation Forest, Autoencoder\n\n\nDimensionality Reduction\nHigh-dimensional features (e.g., pixels)\n2D or 3D projections for analysis or visualization\nHow can I reduce feature space while preserving info?\nHow can I visualize or simplify complex data for human analysis or modeling?\nPCA, t-SNE, UMAP\n\n\nMarket Basket Analysis\nSets of purchased items\nAssociation rules (A & B → C)\nWhat items co-occur frequently in purchases?\nWhich product bundles or cross-sell offers should I promote?\nApriori, FP-Growth\n\n\nWord Embedding\nText corpus\nWord vectors capturing semantic similarity\nWhat are the contextual relationships between words?\nHow can I build a smarter search engine or chatbot that understands language context?\nWord2Vec, GloVe\n\n\nImage Compression\nRaw pixel arrays\nCompressed version of the image\nHow can I represent this image with fewer features?\nHow can I reduce storage or transmission costs for image data?\nAutoencoders",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning II</span>"
    ]
  },
  {
    "objectID": "02-machine-learning-2.html#reinforcement-learning",
    "href": "02-machine-learning-2.html#reinforcement-learning",
    "title": "4  Machine Learning II",
    "section": "4.4 Reinforcement Learning",
    "text": "4.4 Reinforcement Learning\n\n4.4.1 Reinforcement Learning Use Cases\n\n\n\n\n\n\n\n\n\n\n\nUse Case\nSample Inputs\nModel Output Description\nWhat ML Question is Being Answered?\nWhat Business Question is Being Answered?\nExample Algorithm(s)\n\n\n\n\nGame Playing\nGame state (e.g., board, score)\nAction to take\nWhat should I do to win the game?\nHow can I build an AI that outperforms humans or creates adaptive gameplay?\nQ-learning, DQN\n\n\nRobotics & Control\nSensor data (angles, velocities, etc.)\nMovement or control signals\nHow should the agent move next to reach a goal?\nHow can I automate physical tasks like picking, sorting, or navigating?\nPPO, SAC, DDPG\n\n\nAutonomous Vehicles\nSensor input (camera, LIDAR, speed, GPS)\nDriving action\nWhat’s the optimal next driving move?\nHow can I develop a safe and efficient self-driving vehicle system?\nDeep RL + sensor fusion\n\n\nRecommendation Systems\nUser history, preferences, session behavior\nRecommended item\nWhat should I recommend next?\nHow can I increase user retention, engagement, or sales?\nContextual Bandits, RL\n\n\nPortfolio Management\nFinancial indicators, stock prices\nAsset allocation decision\nHow should I invest to maximize return?\nHow can I build an automated trading or portfolio optimization system?\nActor-Critic methods\n\n\nPersonalized Education\nStudent progress and quiz results\nNext learning step\nWhat lesson or content should come next?\nHow can I boost student outcomes by personalizing learning pathways?\nMulti-armed bandits\n\n\nHealthcare Treatment\nPatient history and vitals\nTreatment or intervention strategy\nWhat care plan maximizes long-term patient health?\nHow can I optimize healthcare outcomes while reducing costs and readmissions?\nOff-policy RL, POMDPs",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning II</span>"
    ]
  },
  {
    "objectID": "02-machine-learning-2.html#glossary",
    "href": "02-machine-learning-2.html#glossary",
    "title": "4  Machine Learning II",
    "section": "4.5 Glossary",
    "text": "4.5 Glossary\n\nlogistic (sigmoid) function:\nSoftmax:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning II</span>"
    ]
  },
  {
    "objectID": "03-deep-learning.html",
    "href": "03-deep-learning.html",
    "title": "5  Deep Learning",
    "section": "",
    "text": "5.1 Neural Networks\n“Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder–decoder architecture for language translation.”\n“An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text.” (Raschka (2024))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "03-deep-learning.html#glossary",
    "href": "03-deep-learning.html#glossary",
    "title": "5  Deep Learning",
    "section": "5.2 Glossary",
    "text": "5.2 Glossary\n\nInput Layer: Layer 0. This layer holds the input features. It doesn’t perform any computation — it just passes the input values into the network.\nHidden Layer(s): Layers 1 to n. These are the intermediate layers between input and output. They apply learned weights, biases, and activation functions to transform the data. There may be one or many hidden layers depending on the depth of the network.\nOutput Layer: Layer n+1. This is the final layer that produces the network’s prediction. Its activation function is often task-specific (e.g., softmax for classification, linear for regression).\nNeuron:\nPerceptron:\nMulti-layer Perceptron:\nActivation:\nForward propagation:\nBackpropagation:\nEpoch: One epoch = the model has seen every example in the training dataset once. Training a model involves multiple epochs so it can gradually learn patterns. With each epoch, the model updates its weights using backpropagation, ideally reducing the loss function. If your dataset has 10,000 examples and your batch size is 100, then you’ll have 100 batches per epoch. LLM pretraining will often have just 1–3 epochs over a huge corpora (due to dataset size and overfitting risk)\nTraining example: A sequence of tokens, often 512–8192 in a modern LLM.\nBatch: A set of examples processed together",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "03-deep-learning.html#traditional-presentation-of-logistic-regression",
    "href": "03-deep-learning.html#traditional-presentation-of-logistic-regression",
    "title": "5  Deep Learning",
    "section": "5.3 Traditional Presentation of Logistic Regression",
    "text": "5.3 Traditional Presentation of Logistic Regression\nLogistic regression is commonly introduced as a linear model used for binary classification. Given an input vector \\(x \\in \\mathbb{R}^n\\), the model computes a linear combination of the inputs and passes it through the sigmoid activation function to produce a probability between 0 and 1.\n\n5.3.1 Model Equation\n\\[\n\\hat{y} = \\sigma(w^\\top x + b)\n= \\frac{1}{1 + e^{-(w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b)}}\n\\]\nWhere:\n\n\\(x = [x_1, x_2, \\dots, x_n]^\\top\\) is the input feature vector\n\\(w = [w_1, w_2, \\dots, w_n]^\\top\\) is the weight vector\n\\(b\\) is the bias (intercept) term\n\\(\\sigma(z)\\) is the sigmoid function: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "03-deep-learning.html#matrix-form",
    "href": "03-deep-learning.html#matrix-form",
    "title": "5  Deep Learning",
    "section": "5.4 Matrix Form",
    "text": "5.4 Matrix Form\nWe can express the same model in matrix notation:\n\\[\nz = w^\\top x + b\n\\quad \\text{and} \\quad\n\\hat{y} = \\sigma(z)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "03-deep-learning.html#interpretation",
    "href": "03-deep-learning.html#interpretation",
    "title": "5  Deep Learning",
    "section": "5.5 Interpretation",
    "text": "5.5 Interpretation\nThis model predicts the probability that the output class is 1, given input \\(x\\). It is typically trained using binary cross-entropy loss (also known as log loss):\n\\[\n\\mathcal{L}(y, \\hat{y}) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "03-deep-learning.html#rewriting-logistic-regression-as-a-neural-network",
    "href": "03-deep-learning.html#rewriting-logistic-regression-as-a-neural-network",
    "title": "5  Deep Learning",
    "section": "5.6 Rewriting Logistic Regression as a Neural Network",
    "text": "5.6 Rewriting Logistic Regression as a Neural Network\nNow we observe that this model is mathematically equivalent to a one-layer neural network:\n\nInputs: \\(x_1, x_2, x_3, \\dots\\)\nOne output neuron\nSigmoid activation\nA bias term modeled as a fixed input node with value 1 and a learnable weight\n\nSee the previous section for a diagram and matrix breakdown of this equivalent neural network.\nModels can be in different modes, evaluation mode.\n\n\n\n\n\nflowchart LR\n    %% Input nodes\n    X1((x1))\n    X2((x2))\n    X3((x3))\n    B((1)):::bias\n\n    %% Output node\n    Y((ŷ))\n\n    %% Weighted connections\n    X1 --&gt;|w1| Y\n    X2 --&gt;|w2| Y\n    X3 --&gt;|w3| Y\n    B --&gt;|b| Y\n\n    classDef bias fill:#eee,stroke:#333,stroke-width:2px;\n\n\n\n\n\n\n\n5.6.1 Mathematical Representation\nLogistic regression with a bias term can be interpreted as a neural network with:\n\nInput vector \\(\\, \\tilde{x} \\in \\mathbb{R}^4 \\,\\), including a constant 1 for bias\nWeight vector \\(\\, \\tilde{w} \\in \\mathbb{R}^4 \\,\\)\nSigmoid activation at the output\n\n\n5.6.1.1 Input vector (with bias):\n\\[\n\\tilde{x} =\n\\begin{bmatrix}\n1 \\\\\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}\n\\]\n\n\n5.6.1.2 Weight vector (including bias):\n\\[\n\\tilde{w} =\n\\begin{bmatrix}\nb \\\\\nw_1 \\\\\nw_2 \\\\\nw_3\n\\end{bmatrix}\n\\]\n\n\n5.6.1.3 Linear combination:\n\\[\nz = \\tilde{w}^\\top \\tilde{x}\n= b + w_1 x_1 + w_2 x_2 + w_3 x_3\n\\]\n\n\n5.6.1.4 Sigmoid output:\n\\[\n\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\n\n\n\n\n5.6.2 Summary Table\n\n\n\n\n\n\n\n\n\nElement\nSymbol\nShape\nNotes\n\n\n\n\nInput (with bias)\n\\(\\, \\tilde{x} \\,\\)\n\\(\\, \\mathbb{R}^{4 \\times 1} \\,\\)\n3 features + 1 bias\n\n\nWeights (with bias)\n\\(\\, \\tilde{w} \\,\\)\n\\(\\, \\mathbb{R}^{4 \\times 1} \\,\\)\nlearnable params\n\n\nOutput\n\\(\\, \\hat{y} \\,\\)\n\\(\\, \\mathbb{R} \\,\\)\nscalar probability\n\n\n\nThis is a Shinylive application embedded in a Quarto doc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "03-deep-learning.html#neural-network-architectures",
    "href": "03-deep-learning.html#neural-network-architectures",
    "title": "5  Deep Learning",
    "section": "5.7 Neural Network Architectures",
    "text": "5.7 Neural Network Architectures\nToday, nearly all state-of-the-art AI systems, including ChatGPT, are built around transformer architectures — which themselves rely heavily on feedforward networks as core subcomponents. However, other architectures like CNNs and RNNs continue to play crucial roles in specific areas such as computer vision and on-device speech processing.\n\n\n\n\n\n\n\n\nArchitecture\nDescription\nCommon Use Cases\n\n\n\n\nFeedforward Neural Network (FNN)\nThe simplest type of neural network where data flows in one direction—from input to output—through one or more hidden layers. No memory or recurrence. Often called a Multilayer Perceptron (MLP).\nImage classification (with vector inputs), tabular data prediction, building blocks in LLMs (e.g., transformer feedforward layers)\n\n\nConvolutional Neural Network (CNN)\nUses convolutional layers with local filters and shared weights to process spatial or grid-like data. Often followed by pooling layers to reduce dimensionality.\nImage and video recognition, object detection, facial recognition, medical imaging\n\n\nRecurrent Neural Network (RNN)\nDesigned for sequential data. Uses internal memory (hidden state) to capture dependencies across time steps. Each output depends on previous inputs.\nLanguage modeling, time-series forecasting, speech recognition\n\n\nLong Short-Term Memory (LSTM) / GRU\nVariants of RNNs that solve the vanishing gradient problem. Maintain long-range dependencies using gated mechanisms.\nMachine translation, stock price prediction, chatbot state tracking\n\n\nTransformer\nUses self-attention to weigh relationships between tokens in a sequence. Does not rely on recurrence. Stacked layers often include self-attention + feedforward sub-layers.\nLarge Language Models (GPT, BERT), translation, code generation, question answering\n\n\nAutoencoder\nLearns a compressed (latent) representation of input data and reconstructs it. Composed of an encoder and decoder. Often unsupervised.\nDimensionality reduction, denoising images, anomaly detection\n\n\nGenerative Adversarial Network (GAN)\nConsists of a generator and a discriminator in a game-theoretic setup. The generator creates synthetic data; the discriminator judges real vs. fake.\nImage synthesis, data augmentation, deepfake generation, art creation\n\n\n\n\n\n\n\nRaschka S (2024) Build a large language model (from scratch) (Manning).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "04-llms.html",
    "href": "04-llms.html",
    "title": "6  Large Language Models",
    "section": "",
    "text": "6.1 Glossary",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "04-llms.html#words-represented-as-numbers",
    "href": "04-llms.html#words-represented-as-numbers",
    "title": "6  Large Language Models",
    "section": "6.2 Words represented as numbers",
    "text": "6.2 Words represented as numbers\n2000 - Bag of Words 2013 - Word2Vec 2017 - Attention",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "04-llms.html#key-terms",
    "href": "04-llms.html#key-terms",
    "title": "6  Large Language Models",
    "section": "6.3 Key Terms",
    "text": "6.3 Key Terms\n\nVector:\nMatrix:\nDimension:\nCharacter:\nText string: A sequence of characters forming natural language (e.g., “The quick brown fox”).\nCorpus:\nTokenizer: An algorithm that splits a text string into ‘tokens.’\nTokenization: The process of using a tokenizer to split a text string into smaller units (tokens), typically words or subword fragments.\nToken: A single unit resulting from tokenization, often a word or part of a word.\nVocabulary: A fixed list of all known tokens. Each token is mapped to a unique integer (token ID).\nToken ID: A unique integer assigned to a token by a tokenizer. In Word2Vec, token IDs are not used or exposed. Instead, the token is represented directly as a character string.\nEmbedding Model: An embedding model is a model that transforms each input token—either a character string or a token ID—into a vector of real numbers. The elements of this vector capture aspects of the token’s semantic meaning, syntactic role, and relationships to other tokens in the corpus. These vector representations can then be used as input to downstream machine learning models for tasks such as classification, clustering, translation, or text generation.\n\nWord2Vec – A neural model that creates static word embeddings based on word co-occurrence. Pre-trained versions (such as the Google News model) are available off the shelf using the gensim library.\n\nEmbedding: A vector that represents a token’s learned meaning and context. GPT-2 era models typically use embedding sizes of 768 dimensions, while GPT-3 models use much larger embeddings—up to 12,288 dimensions for the largest model variants (source: Build LLMs from Scratch). Embeddings can be derived from off-the-shelf models like Word2Vec, but modern large language models (LLMs) learn these embeddings during training as part of an integrated process, rather than relying on pre-trained static embeddings. Embeddings can also be created for larger units such as sentences, paragraphs, or entire documents.\nEmbedding Matrix: A matrix in which each row contains the embedding vector for a particular token. It functions as a lookup table to retrieve the embedding for a given token (Word2Vec) or in modern LLMs a token ID. The embedding matrix is a core component of what is commonly referred to as the embedding layer in neural network models.\nEmbedding layer: A layer in a neural network that maps discrete input tokens—typically represented by token IDs—into continuous vector representations by retrieving rows from an embedding matrix. During training, this matrix is randomly initialized and then updated via backpropagation to learn useful representations. After training, the embedding layer functions as a fixed lookup table that outputs meaningful vectors for each token.\nRotary Embedding (RoPE): Positial embedding information is added at the self embedding layer.\nMixture of Experts: variant of tranformation model.\n\nRouter\n\nGenerative vs Representation Models: “Representation models are LLMs that do not generate text but are commonly used for task-specific use cases, like classification, whereas generation models are LLMs that generate text, like GPT models. Although generative models are typically the first thing that comes to mind when thinking about LLMs, there is still much use for representation models.” (Alammar and Grootendorst (2024))\n\n“In other words, the embedding layer is essentially a lookup operation that retrieves rows from the embedding layer’s weight matrix via a token ID.”\n\nTensor:\nShape: The dimensions of a tensor — specifically, the number of elements along each axis.\n\n\n\n\n\n\n\n\n\n\nTensor Type\nRank\nExample Shape\nExample Description\n\n\n\n\nScalar\n0\n()\nA single number (e.g., 5)\n\n\nVector\n1\n(3,)\nA list of numbers (e.g., [5, 2, 7])\n\n\nMatrix\n2\n(3, 4)\nA 2D table (e.g., 3 rows × 4 columns)\n\n\n3D Tensor\n3\n(10, 3, 4)\nA stack of 10 matrices, each 3×4\n\n\nn-D Tensor\nn\n(d1, d2, ..., dn)\nAny n-dimensional array\n\n\n\n\nBackpropogation: The algorithm used to train neural networks by adjusting the model’s weights to reduce error.\nFeedforward: The process of passing input data through a neural network to produce an output. Data flows forward from the input layer through one or more hidden layers to the output layer. At each layer, neurons apply a weighted sum and an activation function (e.g., ReLU, GELU) to produce their outputs. No learning or weight updates happen during this step — it’s just computing the prediction. This is how the model computes predictions, whether during training or inference.\nLogits: “The model outputs, which are commonly referred to as logits”\nDense Vector: A vector in which most or all elements are non-zero and explicitly stored. In NLP, dense vectors (such as embeddings) represent features in a compact, continuous space and are learned during model training.\n\nInput → Feedforward → Prediction ↓ Loss Computation ↓ Backpropagation (Gradients) ↓ Optimization (Weight Update) ↓ Next iteration\nStochastic Gradient Descent:\nBatched Matrix Multiplication:\n– Transformer: Does not use an RRN.\n\nBERT: Encoder only. Good at language translation but not other tasks.\nMasked Language Modeling: Mask some of the input words and predict them.\nGenerative Models: Decoder only models.\nGPT: Generative Pretrained Transformer.\nContext Length\nGPU:\nScaled dot-product attention:\nContext vectors:\nWeight Matrix: Generic term that can refer to any trainable matrix where each element is a weight that gets updated during training.\n\nDuring training, each batch of data goes through a feedforward pass to compute a prediction, then a backpropagation pass to update the model’s weights.\nExcerpt From Build a Large Language Model (From Scratch) Sebastian Raschka This material may be protected by copyright.\n\nInput embedding: The final vector input to the model for each token, typically formed by summing token embeddings with positional (and optional segment) embeddings.\nDetokenization - The process of reconstructing a human-readable string from tokens.\nEmbeddings can be combined\n\nWord, Sentence, Document\n\nVocabulary (Bag of Words): distint list of tokens\nEncoder - The goal of an encoder is to convert input data—such as text—into a numerical representation (embedding) that captures its meaning, structure, and context. In the context of language models, encoders are designed to understand and represent the input text in a way that can be used for downstream tasks like classification, translation, or generation. Mathematically an encoder is often a recurrent neural network.\nDecoder - Goals is to generate language. Takes as inputs, embeddings, and then maps those embeddings to new embeddings, i.e. lanauge translation.\nBag-of-Words\nWord2Vec - A pre-trained model that creates word embeddings. Available off the shelf.\n\nVector Embeddings - Captures the meaning of words. Each Vector (word) embedding represents an aspect of the relationship a given word has with many other words. Each entry in a vector embedding is a number that measures something about the relationship between a word and the word the vector represents. Each vector embedding can have 1024 entries or more. The values of each entry are derived from the perameters of a neural network.\n\n“The main idea behind Word2Vec is that words that appear in similar contexts tend to have similar meanings.” Excerpt From Build a Large Language Model (From Scratch) Sebastian Raschka This material may be protected by copyright.\n\nStatic embeddings. e.g. bank has the same embedding regardless if the sentence is “bank of a river” or “going to the bank”\n\nNeural network\n\nRecurrent Neural Networks - precludes parallelization. This is why the tranformer architecture was so powerful as it allows parallelization.\n\nParameters: “In the context of deep learning and LLMs like GPT, the term “parameters” refers to the trainable weights of the model. ” “These weights are essentially the internal variables of the model that are adjusted and optimized during the training process to minimize a specific loss function.”\nAutoregressive - decoders produce one new token at a time, taking the all previous tokens as input to predict the next best token.\nProjection Matrices\n\nQuery Projection\nKey Projection\nValue Projection\n\nMulti query attention\nGrouped query attention\n\nFeedforward Neural Network\nSelf-attention (encoder???). works by seeing how similar each word is to all of the words in the sentence, including it self. Calculates the simliarity between every every word in the sentence.\n\nTakes the longest and most computation. W\nAttention head\n\nAttention - First introduced in 2014. got big in 2017. Attention allows a model to focus on parts of the input that are relevant to on another. Attend to one another and amplify their signal. Attention selectively determines which words are most important in a given sentence.\nAttention: “weights”, “attends to”, “pays attention to” each input tokens embedding indepentently based on where it is in the input and output. “A way for the model to”pay attention” to different input tokens when processing each token, so it can focus more on the relevant words, even if they’re far away in the sequence.”",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "04-llms.html#scaled-dot-product-attention",
    "href": "04-llms.html#scaled-dot-product-attention",
    "title": "6  Large Language Models",
    "section": "6.4 Scaled Dot-Product Attention",
    "text": "6.4 Scaled Dot-Product Attention\nThe core equation for attention used in Transformers is:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right)V\n\\]\n\n6.4.1 Explanation of Terms\n\n\\(Q\\): Query matrix (sequence length × \\(d_k\\))\n\\(K\\): Key matrix (sequence length × \\(d_k\\))\n\\(V\\): Value matrix (sequence length × \\(d_v\\))\n\\(d_k\\): Dimensionality of the key vectors (used for scaling)\n\n\n\n6.4.2 Intuition\n\nCompute the dot product between \\(Q\\) and \\(K\\): \\(QK^\\top\\)\nScale by \\(\\sqrt{d_k}\\) to control the magnitude\nApply softmax to get attention weights\nMultiply by \\(V\\) to get the final weighted output\n\nThe Terms Query, Key, Value come from database terminology (Starmer (2025))\n\nMasked self-attention (decoder) - removed upper diagonal. Also called Causal self-attention\nCausal self-attention:\nMulti-head attention:\nHidden states of a word can be passed to a decoder. Hidden states are a vector representation.\nBERT 2018 - Encoder only architecture. Forms the basis for most of the embedding models that are popular today for RAG.\nCLS token\nMasked lanaguage modeling. Predict the masked words. Pre-trainiing. Fine tune for downstream tasks.\nGenerative models use a different architecture. Decoders only. GPT-1. Generative pretrained transformer\nContext length, GPT\n\nEach word has a static embedding from Word2Vec. These embeddings are passed to the encoder as a set. The individual embeddings are then combined into a single “context” embedding. The “context” embedding is then passed to the decoder.\nOnce students understand static embeddings (Word2Vec), it becomes much more intuitive to explain: “Now imagine that the same word has a different vector depending on its sentence context — that’s what GPT does.”\nWord2Vec was a milestone in NLP (2013), and understanding it provides insight into how word embeddings evolved from: Bag of Words → TF-IDF → Word2Vec → GloVe → Contextual embeddings (BERT/GPT)\n\nBag of Words and TF-IDF (symbolic → numeric)\nWord2Vec (static embeddings, co-occurrence-based learning)\nGloVe (matrix factorization variant)\nfastText (subword embeddings and OOV handling)\nBERT/GPT embeddings (contextual, dynamic)\nEmbeddings as foundation for downstream tasks\n\n\nLM Head: Token probability calculation. Probability\nDecoding Strategy\n\nGreedy decoding: Temperature = 0\ntop_p: Temperature &gt; 0\n\nTemperature Parameter:\nReinforcement learning from human feedback (RLHF)\nRelevance scoring:\nProjection:\nByte Pair Encoding (BPE): “Builds its vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words. For example, BPE starts with adding all individual single characters to its vocabulary (“a,” “b,” etc.). In the next stage, it merges character combinations that frequently occur together into subwords. For example, “d” and “e” may be merged into the subword “de,” which is common in many English words like “define,” “depend,” “made,” and “hidden.” The merges are determined by a frequency cutoff.”\nLayer normalization: “The main idea behind layer normalization is to adjust the activations (outputs) of a neural network layer to have a mean of 0 and a variance of 1, also known as unit variance. This adjustment speeds up the convergence to effective weights and ensures consistent, reliable training. In GPT-2 and modern transformer architectures, layer normalization is typically applied before and after the multi-head attention module”\nBatch normalization: “normalizes across the batch dimension vs layer normalization which normalizes across the feature dimension”\nShortcut connections, also known as skip or residual connections - “a shortcut connection creates an alternative, shorter path for the gradient to flow through the network by skipping one or more layers, which is achieved by adding the output of one layer to the output of a later” “Shortcut connections are a core building block of very large models such as LLMs, and they will help facilitate more effective training by ensuring consistent gradient flow across layers”\nVanishing gradient: “problem refers to the issue where gradients (which guide weight updates during training) become progressively smaller as they propagate backward through the layers, making it difficult to effectively train earlier layers.”\nLoss Function:\nTransformer Block : “combines multi-head attention, layer normalization, dropout, feed forward layers, and GELU activations” “outputs of the transformer block are vectors of the same dimension as the input, which can then be fed into subsequent layers in an LLM.” “a core structural component of GPT models, combining masked multi-head attention modules with fully connected feed forward networks that use the GELU activation function.”\nWeight Tying:\nSoftmax:\nTraining example:\nBatch:\nOutputs of the layer:\nDropout: “ there are three distinct places where we used dropout layers: the embedding layer, shortcut layer, and multi-head attention module.”",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "04-llms.html#how-llms-work",
    "href": "04-llms.html#how-llms-work",
    "title": "6  Large Language Models",
    "section": "6.5 How LLMs work",
    "text": "6.5 How LLMs work\nTransformers are a typer of neural network.\nUnlike earlier neural networks (like RNNs or CNNs), Transformers rely entirely on attention mechanisms and avoid recurrence, allowing for better parallelization and performance on long sequences.\nTransformers have three components: 1. Word Embedding 2. Positional Encoding - keeps track of word order 3. Attention\nTransformer LLMs generate their output one token at a time.\nTransformer has three major components: 1. Tokenizer 2. Tranformer Block - Most of them computation happens here. This is where the main neural network models live. GPT-3.5 had about 96 transformer blocks. - self attention layer - Relevance scoring - Combining information - Feed forward neural network 3. LM Head - Also a neural network. - Only recieves the final token from the input sequence and then predicts the next word.\nTransformers process their input tokens in parallel. If an input prompt has 16,000 tokens, the model will process this many tokens in parallel.\nKV Caching.\nTime to first token. How",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "04-llms.html#hyperparameters",
    "href": "04-llms.html#hyperparameters",
    "title": "6  Large Language Models",
    "section": "6.6 Hyperparameters",
    "text": "6.6 Hyperparameters\n\n6.6.1 Model Architecture\n\n\n\n\n\n\n\n\nHyperparameter\nDescription\nTypical Values / Notes\n\n\n\n\nvocab_size\nNumber of unique tokens in the vocabulary\n30,000 – 100,000+\n\n\nmax_position_embeddings\nMaximum input sequence length\n128 – 2048+\n\n\nd_model\nEmbedding & hidden size\n128 – 12,288 (GPT-3 uses 12,288)\n\n\nnum_layers / n_layers\nNumber of Transformer blocks\n2 – 96+\n\n\nnum_heads / n_heads\nNumber of attention heads per block\nMust divide evenly into d_model\n\n\nd_ff / ffn_dim\nFeedforward network hidden size\nTypically 4 × d_model\n\n\ndropout_rate\nDropout probability\n0.0 – 0.3\n\n\nactivation_function\nActivation used in FFN\n\"relu\", \"gelu\", \"silu\"\n\n\nlayer_norm_eps\nSmall constant for LayerNorm stability\n1e-12 – 1e-5\n\n\n\n\n\n6.6.2 Attention Mechanism\n\n\n\n\n\n\n\n\nHyperparameter\nDescription\nTypical Values / Notes\n\n\n\n\nattention_dropout\nDropout on attention weights\nHelps regularize training\n\n\nuse_bias\nWhether projection layers have bias\nTrue / False\n\n\nuse_scaled_dot_product\nUse scaled dot-product attention\nUsually True\n\n\nrelative_position_encoding\nUse relative instead of absolute positions\nUsed in Transformer-XL, T5, etc.\n\n\n\n\n\n6.6.3 Training Configuration\n\n\n\n\n\n\n\n\nHyperparameter\nDescription\nTypical Values / Notes\n\n\n\n\nlearning_rate\nInitial learning rate\n1e-5 – 1e-3\n\n\nbatch_size\nExamples per batch\n8 – 2048\n\n\nnum_epochs\nNumber of passes through training data\n3 – 50+\n\n\nweight_decay\nL2 regularization coefficient\n0.0 – 0.1\n\n\ngradient_clip_norm\nClip gradients to this norm\n0.5 – 1.0\n\n\noptimizer\nOptimization algorithm\nAdam, AdamW, AdaFactor, etc.\n\n\nlearning_rate_scheduler\nAdjust learning rate over time\nlinear, cosine, constant, etc.\n\n\nwarmup_steps\nSteps before learning rate decay\n500 – 10,000+\n\n\n\n\n\n6.6.4 Tokenizer & Embeddings\n\n\n\n\n\n\n\n\nHyperparameter\nDescription\nNotes\n\n\n\n\ntokenizer_type\nTokenization algorithm\nBPE, WordPiece, SentencePiece\n\n\nshare_embeddings\nShare encoder & decoder embeddings\nUsed in T5\n\n\n\n\n\n6.6.5 Decoder-Specific\n\n\n\n\n\n\n\n\nHyperparameter\nDescription\nNotes\n\n\n\n\nuse_encoder_decoder\nWhether model includes a decoder\nTrue for T5, translation, etc.\n\n\ndecoder_num_layers\nNumber of decoder layers\nCan differ from encoder\n\n\ncross_attention\nEnables decoder to attend to encoder output\nRequired in encoder-decoder models",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "04-llms.html#understanding-attention",
    "href": "04-llms.html#understanding-attention",
    "title": "6  Large Language Models",
    "section": "6.7 Understanding Attention",
    "text": "6.7 Understanding Attention\nBy computing an attention weight, you are asking the question:\nHow similar is a given token to every other token in the input sequence as measured by the dot product between each pair of embeddings? We then normalized this set of weights using the softmax function which ensure that all the weights sum to 1. for a given token.\n\n\n\n\nAlammar J, Grootendorst M (2024) Hands-on large language models (O’Reilly).\n\n\nStarmer J (2025) Attention in transformers: Concepts and code in PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "05-strategy.html",
    "href": "05-strategy.html",
    "title": "7  AI Strategy",
    "section": "",
    "text": "7.1 The Impacts of AI",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Strategy</span>"
    ]
  },
  {
    "objectID": "05-strategy.html#things-as-they-really-are",
    "href": "05-strategy.html#things-as-they-really-are",
    "title": "7  AI Strategy",
    "section": "7.2 Things as They Really Are",
    "text": "7.2 Things as They Really Are",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Strategy</span>"
    ]
  },
  {
    "objectID": "05-strategy.html#using-ai-for-good",
    "href": "05-strategy.html#using-ai-for-good",
    "title": "7  AI Strategy",
    "section": "7.3 Using AI for Good",
    "text": "7.3 Using AI for Good",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Strategy</span>"
    ]
  },
  {
    "objectID": "90-resources.html",
    "href": "90-resources.html",
    "title": "8  Resources",
    "section": "",
    "text": "8.1 Videos\nLet’s build GPT: from scratch, in code, spelled out by Andrej Karpathy\nNeural Networks Visually by 3Blue1Brown\nThe 35 Year History of LLMs\nhttps://x.com/karpathy/status/1917961248031080455",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "90-resources.html#tokenizers",
    "href": "90-resources.html#tokenizers",
    "title": "8  Resources",
    "section": "8.2 Tokenizers",
    "text": "8.2 Tokenizers\nByte Pair Encoding (BPE) “builds its vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words. For example, BPE starts with adding all individual single characters to its vocabulary (“a,” “b,” etc.). In the next stage, it merges character combinations that frequently occur together into subwords. For example, “d” and “e” may be merged into the subword “de,” which is common in many English words like “define,” “depend,” “made,” and “hidden.” The merges are determined by a frequency cutoff.” (Raschka (2024))\n\n\n\n\n\n\n\n\nTokenizer Type\nUsed In\nDescription\n\n\n\n\nWordPiece\nBERT, DistilBERT, ALBERT\nSplits words into frequent subword units using a greedy longest-match-first algorithm. Handles rare or unknown words by decomposing them into known parts.\n\n\nByte Pair Encoding (BPE)\nGPT-2, GPT-Neo, RoBERTa\nUses a data-driven merge process to combine frequently occurring character pairs into subwords. Efficient and balances vocabulary size with coverage.\n\n\nByte-Level BPE\nGPT-2, GPT-3, GPT-4\nA variant of BPE that operates at the byte level, enabling robust handling of any UTF-8 text (including emojis and accents). No need for pre-tokenization.\n\n\nSentencePiece\nT5, XLNet, some ALBERT versions\nTrains directly on raw text (with or without spaces). Supports BPE or Unigram language model algorithms. Useful for languages without whitespace delimiters.\n\n\nCustom (Hugging Face Tokenizers)\nMany Hugging Face models\nA flexible library for building and using fast, production-ready tokenizers (WordPiece, BPE, Unigram, byte-level) with customizable pre- and post-processing.\n\n\n\nTiktokenizer is a free, browser-based tool that helps you see how language models like GPT-3.5 and GPT-4 break text into tokens—the fundamental units they process. Built using OpenAI’s official tiktoken library, it allows you to input any text, choose a model, and instantly see how many tokens your input uses, along with how the text is split. This is essential for understanding how models interpret prompts, stay within context limits, and calculate usage costs. It’s a practical tool for debugging, optimizing prompts, and learning how AI systems “see” language.\nTokenizer. OpenAI’s large language models process text using tokens, which are common sequences of characters found in a set of text. The models learn to understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "90-resources.html#thought-leaders",
    "href": "90-resources.html#thought-leaders",
    "title": "8  Resources",
    "section": "8.3 Thought Leaders",
    "text": "8.3 Thought Leaders\nJay Alammar\nhttps://jalammar.github.io/\nEnroll for free now: https://bit.ly/4aRnn7Z Github Repo: https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\nAndrej Karpathy https://github.com/karpathy/LLM101n\nWhat I cannot create, I do not understand. -Richard Feynman\nIn this course we will build a Storyteller AI Large Language Model (LLM). Hand in hand, you’ll be able to create, refine and illustrate little stories with the AI. We are going to build everything end-to-end from basics to a functioning web app similar to ChatGPT, from scratch in Python, C and CUDA, and with minimal computer science prerequisites. By the end you should have a relatively deep understanding of AI, LLMs, and deep learning more generally.\nSyllabus\nChapter 01 Bigram Language Model (language modeling) Chapter 02 Micrograd (machine learning, backpropagation) Chapter 03 N-gram model (multi-layer perceptron, matmul, gelu) Chapter 04 Attention (attention, softmax, positional encoder) Chapter 05 Transformer (transformer, residual, layernorm, GPT-2) Chapter 06 Tokenization (minBPE, byte pair encoding) Chapter 07 Optimization (initialization, optimization, AdamW) Chapter 08 Need for Speed I: Device (device, CPU, GPU, …) Chapter 09 Need for Speed II: Precision (mixed precision training, fp16, bf16, fp8, …) Chapter 10 Need for Speed III: Distributed (distributed optimization, DDP, ZeRO) Chapter 11 Datasets (datasets, data loading, synthetic data generation) Chapter 12 Inference I: kv-cache (kv-cache) Chapter 13 Inference II: Quantization (quantization) Chapter 14 Finetuning I: SFT (supervised finetuning SFT, PEFT, LoRA, chat) Chapter 15 Finetuning II: RL (reinforcement learning, RLHF, PPO, DPO) Chapter 16 Deployment (API, web app) Chapter 17 Multimodal (VQVAE, diffusion transformer) Appendix\nFurther topics to work into the progression above:\nProgramming languages: Assembly, C, Python Data types: Integer, Float, String (ASCII, Unicode, UTF-8) Tensor: shapes, views, strides, contiguous, … Deep Learning frameworks: PyTorch, JAX Neural Net Architecture: GPT (1,2,3,4), Llama (RoPE, RMSNorm, GQA), MoE, … Multimodal: Images, Audio, Video, VQVAE, VQGAN, diffusion\n\n\n\n\nRaschka S (2024) Build a large language model (from scratch) (Manning).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "98-faq.html",
    "href": "98-faq.html",
    "title": "9  FAQ",
    "section": "",
    "text": "What does the word projection mean?\n\n\nWhat is the relationship between the size of the vocabulary and the number of dimensions in an embedding vector? When are these the same and when are they not?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>FAQ</span>"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "References",
    "section": "",
    "text": "Alammar J, Grootendorst M (2024) Hands-on\nlarge language models (O’Reilly).\n\n\nRaschka S (2024) Build\na large language model (from scratch) (Manning).\n\n\nStarmer J (2025) Attention in transformers: Concepts and code in\nPyTorch.",
    "crumbs": [
      "References"
    ]
  }
]